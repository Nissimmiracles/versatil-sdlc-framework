/**
 * UltraThink Breakthrough Intelligence System
 * Advanced problem-solving for bottlenecks, struggle loops, and breakthrough solutions
 *
 * Features:
 * - Bottleneck detection and elimination
 * - Struggle loop pattern breaking
 * - Creative solution generation
 * - Cross-domain knowledge transfer
 * - Emergency breakthrough protocols
 * - Meta-level analysis and optimization
 * - Constraint theory application
 * - Breakthrough thinking patterns
 */

import { EventEmitter } from 'events';
import { promises as fs } from 'fs';
import { join } from 'path';

export interface BottleneckAnalysis {
  type: BottleneckType;
  severity: 'low' | 'medium' | 'high' | 'critical';
  location: string;
  impact: BottleneckImpact;
  rootCauses: RootCause[];
  solutions: BreakthroughSolution[];
  urgency: number;
  estimatedResolutionTime: number;
  conflictingConstraints: string[];
}

export enum BottleneckType {
  PERFORMANCE = 'performance',
  DEVELOPMENT_VELOCITY = 'development_velocity',
  DECISION_PARALYSIS = 'decision_paralysis',
  RESOURCE_CONSTRAINT = 'resource_constraint',
  TECHNICAL_DEBT = 'technical_debt',
  KNOWLEDGE_GAP = 'knowledge_gap',
  COMMUNICATION = 'communication',
  PROCESS_INEFFICIENCY = 'process_inefficiency',
  COGNITIVE_OVERLOAD = 'cognitive_overload',
  INTEGRATION_COMPLEXITY = 'integration_complexity'
}

export interface BottleneckImpact {
  developmentSpeed: number; // -100 to 100
  teamMorale: number;
  codeQuality: number;
  deliveryTimeline: number;
  resourceUtilization: number;
  innovation: number;
  costMultiplier: number;
}

export interface RootCause {
  category: 'technical' | 'process' | 'human' | 'organizational' | 'external';
  description: string;
  confidence: number;
  dependencies: string[];
  historicalFrequency: number;
  solutionComplexity: 'simple' | 'moderate' | 'complex' | 'systemic';
}

export interface StruggleLoop {
  id: string;
  pattern: StrugglePattern;
  iterations: number;
  duration: number;
  energyDrain: number;
  confidence: number;
  breakthroughProbability: number;
  alternativeApproaches: AlternativeApproach[];
  exitStrategies: ExitStrategy[];
}

export enum StrugglePattern {
  REPEATED_FAILED_ATTEMPTS = 'repeated_failed_attempts',
  ANALYSIS_PARALYSIS = 'analysis_paralysis',
  OVERCOMPLICATED_SOLUTION = 'overcomplicated_solution',
  MISSING_PERSPECTIVE = 'missing_perspective',
  WRONG_PROBLEM_DEFINITION = 'wrong_problem_definition',
  INSUFFICIENT_KNOWLEDGE = 'insufficient_knowledge',
  COGNITIVE_BIAS = 'cognitive_bias',
  RESOURCE_THRASHING = 'resource_thrashing',
  PERFECTIONISM_TRAP = 'perfectionism_trap',
  SCOPE_CREEP = 'scope_creep'
}

export interface AlternativeApproach {
  name: string;
  description: string;
  paradigmShift: string;
  successProbability: number;
  riskLevel: 'low' | 'medium' | 'high';
  resourceRequirement: string;
  timeToSolution: number;
  innovationLevel: number;
  examples: string[];
}

export interface ExitStrategy {
  name: string;
  description: string;
  conditions: string[];
  actions: string[];
  recoverabilty: number;
  learningValue: number;
}

export interface BreakthroughSolution {
  id: string;
  name: string;
  type: SolutionType;
  approach: SolutionApproach;
  description: string;
  implementation: ImplementationPlan;
  constraints: Constraint[];
  benefits: Benefit[];
  risks: Risk[];
  confidence: number;
  novelty: number;
  elegance: number;
  sustainability: number;
  scalability: number;
}

export enum SolutionType {
  TECHNICAL_INNOVATION = 'technical_innovation',
  PROCESS_OPTIMIZATION = 'process_optimization',
  ARCHITECTURAL_CHANGE = 'architectural_change',
  TOOL_INTRODUCTION = 'tool_introduction',
  METHODOLOGY_SHIFT = 'methodology_shift',
  RESOURCE_REALLOCATION = 'resource_reallocation',
  CONSTRAINT_REMOVAL = 'constraint_removal',
  PARADIGM_CHANGE = 'paradigm_change',
  SIMPLIFICATION = 'simplification',
  AUTOMATION = 'automation'
}

export enum SolutionApproach {
  INCREMENTAL = 'incremental',
  REVOLUTIONARY = 'revolutionary',
  HYBRID = 'hybrid',
  EXPERIMENTAL = 'experimental',
  PROVEN = 'proven',
  EMERGENT = 'emergent'
}

export interface ImplementationPlan {
  phases: ImplementationPhase[];
  prerequisites: string[];
  milestones: Milestone[];
  rollbackPlan: string[];
  successMetrics: string[];
  monitoringStrategy: string;
}

export interface ImplementationPhase {
  name: string;
  description: string;
  duration: number;
  resources: string[];
  deliverables: string[];
  risks: string[];
  dependencies: string[];
}

export interface Milestone {
  name: string;
  description: string;
  successCriteria: string[];
  validationMethod: string;
  timeframe: number;
}

export interface Constraint {
  type: 'technical' | 'resource' | 'time' | 'regulatory' | 'organizational';
  description: string;
  severity: number;
  flexibility: number;
  workarounds: string[];
}

export interface Benefit {
  category: 'performance' | 'quality' | 'productivity' | 'innovation' | 'maintainability';
  description: string;
  quantification: string;
  timeline: 'immediate' | 'short_term' | 'medium_term' | 'long_term';
  confidence: number;
}

export interface Risk {
  description: string;
  probability: number;
  impact: number;
  mitigation: string[];
  contingency: string[];
}

export interface BreakthroughInsight {
  type: InsightType;
  description: string;
  source: InsightSource;
  confidence: number;
  applicability: number;
  novelty: number;
  actionability: string[];
  crossDomainAnalogy?: string;
  contradictionResolution?: string;
}

export enum InsightType {
  PATTERN_RECOGNITION = 'pattern_recognition',
  CONSTRAINT_IDENTIFICATION = 'constraint_identification',
  ASSUMPTION_CHALLENGE = 'assumption_challenge',
  CROSS_DOMAIN_TRANSFER = 'cross_domain_transfer',
  SIMPLIFICATION_OPPORTUNITY = 'simplification_opportunity',
  RESOURCE_OPTIMIZATION = 'resource_optimization',
  WORKFLOW_IMPROVEMENT = 'workflow_improvement',
  TECHNOLOGICAL_LEAP = 'technological_leap',
  PARADIGM_SHIFT = 'paradigm_shift',
  CONTRADICTION_RESOLUTION = 'contradiction_resolution'
}

export enum InsightSource {
  PATTERN_ANALYSIS = 'pattern_analysis',
  CROSS_PROJECT_LEARNING = 'cross_project_learning',
  COMMUNITY_WISDOM = 'community_wisdom',
  EXPERT_KNOWLEDGE = 'expert_knowledge',
  HISTORICAL_DATA = 'historical_data',
  SIMULATION = 'simulation',
  CONSTRAINT_THEORY = 'constraint_theory',
  CREATIVE_ALGORITHMS = 'creative_algorithms',
  BIOMIMICRY = 'biomimicry',
  ANALOGICAL_REASONING = 'analogical_reasoning'
}

export interface MetaAnalysis {
  teamDynamics: TeamDynamicsAnalysis;
  processEfficiency: ProcessEfficiencyAnalysis;
  toolEffectiveness: ToolEffectivenessAnalysis;
  knowledgeGaps: KnowledgeGapAnalysis;
  cognitiveLoad: CognitiveLoadAnalysis;
  innovationIndex: InnovationIndexAnalysis;
  collaborationQuality: CollaborationQualityAnalysis;
}

export interface TeamDynamicsAnalysis {
  communicationPatterns: string[];
  decisionMakingSpeed: number;
  conflictResolutionEfficiency: number;
  knowledgeSharing: number;
  psychologicalSafety: number;
  creativityLevel: number;
  burnoutRisk: number;
  skillComplementarity: number;
}

export interface ProcessEfficiencyAnalysis {
  bottlenecks: string[];
  redundancies: string[];
  automationOpportunities: string[];
  feedbackLoops: string[];
  qualityGates: string[];
  cycleTime: number;
  throughput: number;
  errorRate: number;
}

export interface ToolEffectivenessAnalysis {
  utilizationRate: Record<string, number>;
  learningCurve: Record<string, number>;
  integrationQuality: Record<string, number>;
  maintenanceOverhead: Record<string, number>;
  alternatives: Record<string, string[]>;
  recommendations: string[];
}

export interface KnowledgeGapAnalysis {
  criticalGaps: string[];
  learningPriorities: string[];
  expertiseDistribution: Record<string, number>;
  documentationQuality: number;
  knowledgeTransferRate: number;
  externalDependencies: string[];
}

export interface CognitiveLoadAnalysis {
  complexityLevel: number;
  contextSwitching: number;
  informationOverload: number;
  multitaskingPenalty: number;
  focusFragmentation: number;
  simplificationOpportunities: string[];
}

export interface InnovationIndexAnalysis {
  experimentationRate: number;
  riskTolerance: number;
  ideaGeneration: number;
  implementationSpeed: number;
  learningVelocity: number;
  adaptabilityScore: number;
}

export interface CollaborationQualityAnalysis {
  syncVsAsync: number;
  meetingEfficiency: number;
  documentationSharing: number;
  crossFunctionalAlignment: number;
  feedbackQuality: number;
  decisionTransparency: number;
}

export interface EmergencyProtocol {
  name: string;
  triggerConditions: string[];
  urgencyLevel: 'high' | 'critical' | 'emergency';
  responseTime: number;
  actions: EmergencyAction[];
  resources: string[];
  escalationPath: string[];
  successCriteria: string[];
}

export interface EmergencyAction {
  priority: number;
  description: string;
  owner: string;
  duration: number;
  dependencies: string[];
  rollbackOption: boolean;
}

export class UltraThinkBreakthroughSystem extends EventEmitter {
  private bottlenecks: Map<string, BottleneckAnalysis> = new Map();
  private struggleLoops: Map<string, StruggleLoop> = new Map();
  private solutions: Map<string, BreakthroughSolution> = new Map();
  private insights: BreakthroughInsight[] = [];
  private emergencyProtocols: Map<string, EmergencyProtocol> = new Map();
  private metaAnalysisHistory: MetaAnalysis[] = [];
  private breakthroughPatterns: Map<string, any> = new Map();

  constructor() {
    super();
    this.initializeEmergencyProtocols();
    this.initializeBreakthroughPatterns();
  }

  async performUltraThinkAnalysis(projectPath: string): Promise<{
    bottlenecks: BottleneckAnalysis[];
    struggleLoops: StruggleLoop[];
    breakthroughSolutions: BreakthroughSolution[];
    insights: BreakthroughInsight[];
    metaAnalysis: MetaAnalysis;
    emergencyRecommendations: string[];
  }> {
    try {
      this.emit('ultrathink_started', { projectPath });

      // Phase 1: Deep Bottleneck Detection
      const bottlenecks = await this.detectAllBottlenecks(projectPath);

      // Phase 2: Struggle Loop Analysis
      const struggleLoops = await this.analyzeStruggleLoops(projectPath);

      // Phase 3: Meta-Level System Analysis
      const metaAnalysis = await this.performMetaAnalysis(projectPath);

      // Phase 4: Breakthrough Solution Generation
      const breakthroughSolutions = await this.generateBreakthroughSolutions(
        bottlenecks,
        struggleLoops,
        metaAnalysis
      );

      // Phase 5: Insight Synthesis
      const insights = await this.synthesizeBreakthroughInsights(
        bottlenecks,
        struggleLoops,
        breakthroughSolutions,
        metaAnalysis
      );

      // Phase 6: Emergency Assessment
      const emergencyRecommendations = await this.assessEmergencyNeeds(
        bottlenecks,
        struggleLoops,
        metaAnalysis
      );

      const result = {
        bottlenecks,
        struggleLoops,
        breakthroughSolutions,
        insights,
        metaAnalysis,
        emergencyRecommendations
      };

      this.emit('ultrathink_completed', {
        projectPath,
        bottleneckCount: bottlenecks.length,
        solutionCount: breakthroughSolutions.length,
        insightCount: insights.length
      });

      return result;

    } catch (error) {
      this.emit('error', {
        operation: 'performUltraThinkAnalysis',
        error: error instanceof Error ? error.message : String(error)
      });
      throw error;
    }
  }

  private async detectAllBottlenecks(projectPath: string): Promise<BottleneckAnalysis[]> {
    const bottlenecks: BottleneckAnalysis[] = [];

    // Performance Bottlenecks
    bottlenecks.push(...await this.detectPerformanceBottlenecks(projectPath));

    // Development Velocity Bottlenecks
    bottlenecks.push(...await this.detectVelocityBottlenecks(projectPath));

    // Decision Paralysis Detection
    bottlenecks.push(...await this.detectDecisionParalysis(projectPath));

    // Resource Constraint Analysis
    bottlenecks.push(...await this.detectResourceConstraints(projectPath));

    // Technical Debt Bottlenecks
    bottlenecks.push(...await this.detectTechnicalDebtBottlenecks(projectPath));

    // Knowledge Gap Bottlenecks
    bottlenecks.push(...await this.detectKnowledgeGaps(projectPath));

    // Communication Bottlenecks
    bottlenecks.push(...await this.detectCommunicationBottlenecks(projectPath));

    // Process Inefficiency Bottlenecks
    bottlenecks.push(...await this.detectProcessBottlenecks(projectPath));

    return bottlenecks.sort((a, b) => b.urgency - a.urgency);
  }

  private async analyzeStruggleLoops(projectPath: string): Promise<StruggleLoop[]> {
    const loops: StruggleLoop[] = [];

    // Analyze git commit patterns for repeated attempts
    loops.push(...await this.detectRepeatedFailurePatterns(projectPath));

    // Detect analysis paralysis from issue/PR patterns
    loops.push(...await this.detectAnalysisParalysis(projectPath));

    // Identify overcomplicated solutions
    loops.push(...await this.detectOvercomplication(projectPath));

    // Find missing perspective patterns
    loops.push(...await this.detectMissingPerspectives(projectPath));

    // Detect wrong problem definition
    loops.push(...await this.detectWrongProblemDefinition(projectPath));

    // Identify knowledge insufficiency loops
    loops.push(...await this.detectKnowledgeLoops(projectPath));

    // Detect cognitive bias patterns
    loops.push(...await this.detectCognitiveBiases(projectPath));

    // Resource thrashing detection
    loops.push(...await this.detectResourceThrashing(projectPath));

    return loops;
  }

  private async performMetaAnalysis(projectPath: string): Promise<MetaAnalysis> {
    return {
      teamDynamics: await this.analyzeTeamDynamics(projectPath),
      processEfficiency: await this.analyzeProcessEfficiency(projectPath),
      toolEffectiveness: await this.analyzeToolEffectiveness(projectPath),
      knowledgeGaps: await this.analyzeKnowledgeGaps(projectPath),
      cognitiveLoad: await this.analyzeCognitiveLoad(projectPath),
      innovationIndex: await this.analyzeInnovationIndex(projectPath),
      collaborationQuality: await this.analyzeCollaborationQuality(projectPath)
    };
  }

  private async generateBreakthroughSolutions(
    bottlenecks: BottleneckAnalysis[],
    struggleLoops: StruggleLoop[],
    metaAnalysis: MetaAnalysis
  ): Promise<BreakthroughSolution[]> {
    const solutions: BreakthroughSolution[] = [];

    for (const bottleneck of bottlenecks) {
      // Generate solutions using different approaches
      solutions.push(...await this.generateConstraintTheorySolutions(bottleneck));
      solutions.push(...await this.generateCrossdomainSolutions(bottleneck));
      solutions.push(...await this.generateSimplificationSolutions(bottleneck));
      solutions.push(...await this.generateAutomationSolutions(bottleneck));
      solutions.push(...await this.generateParadigmShiftSolutions(bottleneck));
    }

    for (const loop of struggleLoops) {
      solutions.push(...await this.generateLoopBreakingSolutions(loop));
    }

    // Meta-level solutions
    solutions.push(...await this.generateMetaLevelSolutions(metaAnalysis));

    // Apply solution evaluation and ranking
    return this.rankAndRefineBreakthroughSolutions(solutions);
  }

  private async synthesizeBreakthroughInsights(
    bottlenecks: BottleneckAnalysis[],
    struggleLoops: StruggleLoop[],
    solutions: BreakthroughSolution[],
    metaAnalysis: MetaAnalysis
  ): Promise<BreakthroughInsight[]> {
    const insights: BreakthroughInsight[] = [];

    // Pattern recognition insights
    insights.push(...await this.generatePatternInsights(bottlenecks, struggleLoops));

    // Constraint identification insights
    insights.push(...await this.generateConstraintInsights(bottlenecks));

    // Assumption challenge insights
    insights.push(...await this.generateAssumptionChallenges(solutions));

    // Cross-domain transfer insights
    insights.push(...await this.generateCrossDomainInsights(solutions));

    // Simplification insights
    insights.push(...await this.generateSimplificationInsights(metaAnalysis));

    // Contradiction resolution insights
    insights.push(...await this.generateContradictionInsights(bottlenecks));

    return insights.sort((a, b) => (b.confidence * b.actionability.length) - (a.confidence * a.actionability.length));
  }

  private async assessEmergencyNeeds(
    bottlenecks: BottleneckAnalysis[],
    struggleLoops: StruggleLoop[],
    metaAnalysis: MetaAnalysis
  ): Promise<string[]> {
    const recommendations: string[] = [];

    // Critical bottlenecks requiring immediate attention
    const criticalBottlenecks = bottlenecks.filter(b => b.severity === 'critical');
    for (const bottleneck of criticalBottlenecks) {
      recommendations.push(`ðŸš¨ CRITICAL: ${bottleneck.type} causing ${bottleneck.impact.developmentSpeed}% speed reduction`);
    }

    // High-energy struggle loops
    const severeLoops = struggleLoops.filter(l => l.energyDrain > 0.8);
    for (const loop of severeLoops) {
      recommendations.push(`âš¡ URGENT: Break ${loop.pattern} loop (${loop.iterations} iterations, ${loop.energyDrain * 100}% energy drain)`);
    }

    // Team burnout risks
    if (metaAnalysis.teamDynamics.burnoutRisk > 0.7) {
      recommendations.push(`ðŸ§  TEAM: High burnout risk detected (${metaAnalysis.teamDynamics.burnoutRisk * 100}%)`);
    }

    // Process collapse risks
    if (metaAnalysis.processEfficiency.errorRate > 0.3) {
      recommendations.push(`ðŸ”§ PROCESS: Error rate critical (${metaAnalysis.processEfficiency.errorRate * 100}%)`);
    }

    return recommendations;
  }

  // Specialized bottleneck detection methods
  private async detectPerformanceBottlenecks(projectPath: string): Promise<BottleneckAnalysis[]> {
    const bottlenecks: BottleneckAnalysis[] = [];
    try {
      const { execSync } = require('child_process');

      // Search for performance-related keywords in code
      const performanceKeywords = ['slow', 'performance', 'timeout', 'lag', 'optimize', 'bottleneck'];
      let performanceCommentCount = 0;

      for (const keyword of performanceKeywords) {
        try {
          const result = execSync(
            `git grep -i "${keyword}" -- "*.ts" "*.tsx" "*.js" "*.jsx" | wc -l`,
            { cwd: projectPath, encoding: 'utf8' }
          ).trim();
          performanceCommentCount += parseInt(result) || 0;
        } catch {
          // Keyword not found - continue
        }
      }

      if (performanceCommentCount > 10) {
        bottlenecks.push({
          type: BottleneckType.PERFORMANCE,
          severity: performanceCommentCount > 30 ? 'high' : 'medium',
          location: `Codebase-wide (${performanceCommentCount} performance-related comments)`,
          impact: {
            developmentSpeed: -25,
            teamMorale: -15,
            codeQuality: -30,
            deliveryTimeline: -20,
            resourceUtilization: -35,
            innovation: -10,
            costMultiplier: 1.6
          },
          rootCauses: [
            {
              category: 'technical',
              description: `High density of performance-related comments/TODOs (${performanceCommentCount} instances)`,
              confidence: 0.75,
              dependencies: ['code-quality', 'architecture'],
              historicalFrequency: 0.5,
              solutionComplexity: 'complex'
            }
          ],
          solutions: [],
          urgency: performanceCommentCount > 30 ? 8.0 : 6.5,
          estimatedResolutionTime: 7 * 24 * 60 * 60 * 1000,
          conflictingConstraints: ['performance vs development speed']
        });
      }

      // Check for performance-related commits
      try {
        const perfCommits = execSync(
          'git log --since="90 days ago" --grep="performance\\|slow\\|optimize\\|speed" --oneline | wc -l',
          { cwd: projectPath, encoding: 'utf8' }
        ).trim();

        const perfCommitCount = parseInt(perfCommits) || 0;

        if (perfCommitCount > 5) {
          bottlenecks.push({
            type: BottleneckType.PERFORMANCE,
            severity: 'medium',
            location: `Recent commit history (${perfCommitCount} performance fixes in 90 days)`,
            impact: {
              developmentSpeed: -20,
              teamMorale: -10,
              codeQuality: -25,
              deliveryTimeline: -15,
              resourceUtilization: -30,
              innovation: -15,
              costMultiplier: 1.4
            },
            rootCauses: [
              {
                category: 'technical',
                description: `Recurring performance issues requiring ${perfCommitCount} fixes in 90 days`,
                confidence: 0.8,
                dependencies: ['system-architecture', 'performance-monitoring'],
                historicalFrequency: 0.6,
                solutionComplexity: 'complex'
              }
            ],
            solutions: [],
            urgency: 7.0,
            estimatedResolutionTime: 14 * 24 * 60 * 60 * 1000,
            conflictingConstraints: ['quick fixes vs architectural refactoring']
          });
        }
      } catch {
        // No performance commits found or git log failed
      }

      // Check for large files (potential bundle bloat)
      try {
        const largeFiles = execSync(
          'find . -type f \\( -name "*.js" -o -name "*.ts" -o -name "*.tsx" \\) -size +100k | wc -l',
          { cwd: projectPath, encoding: 'utf8' }
        ).trim();

        const largeFileCount = parseInt(largeFiles) || 0;

        if (largeFileCount > 5) {
          bottlenecks.push({
            type: BottleneckType.PERFORMANCE,
            severity: 'medium',
            location: `Bundle size (${largeFileCount} files > 100KB)`,
            impact: {
              developmentSpeed: -15,
              teamMorale: -5,
              codeQuality: -20,
              deliveryTimeline: -10,
              resourceUtilization: -25,
              innovation: -5,
              costMultiplier: 1.3
            },
            rootCauses: [
              {
                category: 'technical',
                description: `${largeFileCount} large source files indicate potential bundle bloat`,
                confidence: 0.7,
                dependencies: ['build-system', 'code-splitting'],
                historicalFrequency: 0.4,
                solutionComplexity: 'moderate'
              }
            ],
            solutions: [],
            urgency: 5.5,
            estimatedResolutionTime: 5 * 24 * 60 * 60 * 1000,
            conflictingConstraints: ['code organization vs bundle size']
          });
        }
      } catch {
        // find command failed - skip
      }

    } catch (error) {
      // Not a git repo or commands unavailable - silent skip
    }
    return bottlenecks;
  }

  private async detectVelocityBottlenecks(projectPath: string): Promise<BottleneckAnalysis[]> {
    const bottlenecks: BottleneckAnalysis[] = [];
    try {
      const { execSync } = require('child_process');
      const gitLog = execSync('git log --since="30 days ago" --pretty=format:"%ad" --date=short', {
        cwd: projectPath,
        encoding: 'utf8'
      }).trim();

      if (!gitLog) return bottlenecks;

      const commits = gitLog.split('\n').filter(Boolean);
      const avgCommitsPerDay = commits.length / 30;

      if (avgCommitsPerDay < 1) {
        bottlenecks.push({
          type: BottleneckType.DEVELOPMENT_VELOCITY,
          severity: 'high',
          location: `Project-wide commit velocity`,
          impact: {
            developmentSpeed: -50,
            teamMorale: -20,
            codeQuality: -10,
            deliveryTimeline: -60,
            resourceUtilization: -30,
            innovation: -25,
            costMultiplier: 1.8
          },
          rootCauses: [
            {
              category: 'process',
              description: `Low commit velocity: ${avgCommitsPerDay.toFixed(2)} commits/day (target: 3+)`,
              confidence: 0.85,
              dependencies: ['development-process', 'team-capacity'],
              historicalFrequency: 0.6,
              solutionComplexity: 'moderate'
            }
          ],
          solutions: [],
          urgency: 8.0,
          estimatedResolutionTime: 14 * 24 * 60 * 60 * 1000, // 2 weeks
          conflictingConstraints: ['velocity vs quality', 'speed vs thoroughness']
        });
      }

      // Check for commit gaps
      const commitsByDay: Record<string, number> = {};
      commits.forEach(date => {
        commitsByDay[date] = (commitsByDay[date] || 0) + 1;
      });

      const dates = Object.keys(commitsByDay).sort();
      for (let i = 1; i < dates.length; i++) {
        const daysDiff = Math.floor(
          (new Date(dates[i]).getTime() - new Date(dates[i - 1]).getTime()) / (1000 * 60 * 60 * 24)
        );
        if (daysDiff > 3) {
          bottlenecks.push({
            type: BottleneckType.DEVELOPMENT_VELOCITY,
            severity: 'medium',
            location: `Development continuity (${dates[i - 1]} to ${dates[i]})`,
            impact: {
              developmentSpeed: -30,
              teamMorale: -15,
              codeQuality: 0,
              deliveryTimeline: -40,
              resourceUtilization: -20,
              innovation: -10,
              costMultiplier: 1.4
            },
            rootCauses: [
              {
                category: 'process',
                description: `${daysDiff}-day commit gap detected`,
                confidence: 0.9,
                dependencies: ['work-continuity', 'blockers'],
                historicalFrequency: 0.4,
                solutionComplexity: 'simple'
              }
            ],
            solutions: [],
            urgency: 6.5,
            estimatedResolutionTime: 7 * 24 * 60 * 60 * 1000, // 1 week
            conflictingConstraints: []
          });
          break; // Only report first significant gap
        }
      }
    } catch (error) {
      // Not a git repo or git not available - silent skip
    }
    return bottlenecks;
  }

  private async detectDecisionParalysis(projectPath: string): Promise<BottleneckAnalysis[]> {
    const bottlenecks: BottleneckAnalysis[] = [];
    try {
      const { execSync } = require('child_process');

      // Check TODO/FIXME density
      const todoCount = parseInt(execSync(
        'git grep -i "TODO\\|FIXME\\|XXX\\|HACK" -- "*.ts" "*.tsx" "*.js" "*.jsx" | wc -l',
        { cwd: projectPath, encoding: 'utf8' }
      ).trim());

      const fileCount = parseInt(execSync(
        'find . -type f \\( -name "*.ts" -o -name "*.tsx" -o -name "*.js" -o -name "*.jsx" \\) | wc -l',
        { cwd: projectPath, encoding: 'utf8' }
      ).trim());

      const todoPerFile = todoCount / Math.max(fileCount, 1);

      if (todoPerFile > 2) {
        bottlenecks.push({
          type: BottleneckType.DECISION_PARALYSIS,
          severity: 'high',
          location: 'Codebase-wide deferred decisions',
          impact: {
            developmentSpeed: -40,
            teamMorale: -30,
            codeQuality: -50,
            deliveryTimeline: -45,
            resourceUtilization: -25,
            innovation: -20,
            costMultiplier: 1.6
          },
          rootCauses: [
            {
              category: 'process',
              description: `High TODO density: ${todoPerFile.toFixed(1)} per file (${todoCount} total)`,
              confidence: 0.8,
              dependencies: ['decision-making', 'technical-planning'],
              historicalFrequency: 0.7,
              solutionComplexity: 'moderate'
            }
          ],
          solutions: [],
          urgency: 7.5,
          estimatedResolutionTime: 21 * 24 * 60 * 60 * 1000, // 3 weeks
          conflictingConstraints: ['speed vs completeness']
        });
      }

      // Check stale branches
      const branches = execSync('git branch -r --format="%(refname:short) %(committerdate:relative)"', {
        cwd: projectPath,
        encoding: 'utf8'
      }).trim().split('\n');

      const staleBranches = branches.filter(b => b.includes('month') || b.includes('year')).length;

      if (staleBranches > 3) {
        bottlenecks.push({
          type: BottleneckType.DECISION_PARALYSIS,
          severity: 'medium',
          location: 'Branch management',
          impact: {
            developmentSpeed: -25,
            teamMorale: -20,
            codeQuality: -15,
            deliveryTimeline: -30,
            resourceUtilization: -20,
            innovation: -10,
            costMultiplier: 1.3
          },
          rootCauses: [
            {
              category: 'process',
              description: `${staleBranches} stale branches (pending merge decisions)`,
              confidence: 0.75,
              dependencies: ['code-review', 'merge-policy'],
              historicalFrequency: 0.5,
              solutionComplexity: 'simple'
            }
          ],
          solutions: [],
          urgency: 6.0,
          estimatedResolutionTime: 7 * 24 * 60 * 60 * 1000,
          conflictingConstraints: []
        });
      }
    } catch (error) {
      // Silent skip
    }
    return bottlenecks;
  }

  private async detectResourceConstraints(projectPath: string): Promise<BottleneckAnalysis[]> {
    const bottlenecks: BottleneckAnalysis[] = [];
    try {
      const { execSync } = require('child_process');

      const contributors = execSync(
        'git log --since="3 months ago" --format="%an" | sort | uniq -c | sort -rn',
        { cwd: projectPath, encoding: 'utf8' }
      ).trim().split('\n').filter(Boolean);

      const commitCounts = contributors.map(line => {
        const match = line.trim().match(/^(\d+)/);
        return match ? parseInt(match[1]) : 0;
      });

      if (commitCounts.length > 0) {
        const totalCommits = commitCounts.reduce((a, b) => a + b, 0);
        const topContributorPercent = commitCounts[0] / totalCommits;

        if (topContributorPercent > 0.8 && commitCounts.length > 1) {
          bottlenecks.push({
            type: BottleneckType.RESOURCE_CONSTRAINT,
            severity: 'high',
            location: 'Team capacity distribution',
            impact: {
              developmentSpeed: -45,
              teamMorale: -35,
              codeQuality: -20,
              deliveryTimeline: -50,
              resourceUtilization: -60,
              innovation: -30,
              costMultiplier: 2.0
            },
            rootCauses: [
              {
                category: 'organizational',
                description: `Single contributor creates ${(topContributorPercent * 100).toFixed(0)}% of commits (bus factor: 1)`,
                confidence: 0.9,
                dependencies: ['team-size', 'knowledge-distribution'],
                historicalFrequency: 0.8,
                solutionComplexity: 'complex'
              }
            ],
            solutions: [],
            urgency: 9.0,
            estimatedResolutionTime: 90 * 24 * 60 * 60 * 1000, // 3 months
            conflictingConstraints: ['cost vs capacity', 'speed vs training']
          });
        }

        if (commitCounts.length < 3) {
          bottlenecks.push({
            type: BottleneckType.RESOURCE_CONSTRAINT,
            severity: 'medium',
            location: 'Team size',
            impact: {
              developmentSpeed: -35,
              teamMorale: -25,
              codeQuality: -15,
              deliveryTimeline: -40,
              resourceUtilization: -50,
              innovation: -20,
              costMultiplier: 1.7
            },
            rootCauses: [
              {
                category: 'organizational',
                description: `Only ${commitCounts.length} active contributor(s) in last 3 months`,
                confidence: 0.85,
                dependencies: ['hiring', 'team-capacity'],
                historicalFrequency: 0.6,
                solutionComplexity: 'systemic'
              }
            ],
            solutions: [],
            urgency: 7.5,
            estimatedResolutionTime: 60 * 24 * 60 * 60 * 1000,
            conflictingConstraints: ['budget vs capacity']
          });
        }
      }
    } catch (error) {
      // Silent skip
    }
    return bottlenecks;
  }

  private async detectTechnicalDebtBottlenecks(projectPath: string): Promise<BottleneckAnalysis[]> {
    const bottlenecks: BottleneckAnalysis[] = [];
    try {
      const { execSync } = require('child_process');
      const fs = require('fs');
      const path = require('path');

      // Check for large files
      const largeFileCount = parseInt(execSync(
        'find . -type f \\( -name "*.ts" -o -name "*.tsx" -o -name "*.js" -o -name "*.jsx" \\) -exec wc -l {} \\; | awk \'$1 > 500\' | wc -l',
        { cwd: projectPath, encoding: 'utf8' }
      ).trim());

      if (largeFileCount > 10) {
        bottlenecks.push({
          type: BottleneckType.TECHNICAL_DEBT,
          severity: 'high',
          location: 'Code complexity',
          impact: {
            developmentSpeed: -40,
            teamMorale: -25,
            codeQuality: -60,
            deliveryTimeline: -35,
            resourceUtilization: -30,
            innovation: -45,
            costMultiplier: 1.9
          },
          rootCauses: [
            {
              category: 'technical',
              description: `${largeFileCount} files exceed 500 lines (complexity debt)`,
              confidence: 0.85,
              dependencies: ['refactoring', 'module-design'],
              historicalFrequency: 0.7,
              solutionComplexity: 'complex'
            }
          ],
          solutions: [],
          urgency: 7.0,
          estimatedResolutionTime: 30 * 24 * 60 * 60 * 1000,
          conflictingConstraints: ['refactor time vs new features']
        });
      }

      // Check test coverage
      const coverageFile = path.join(projectPath, 'coverage/coverage-summary.json');
      if (fs.existsSync(coverageFile)) {
        const coverage = JSON.parse(fs.readFileSync(coverageFile, 'utf8'));
        const totalCoverage = coverage.total?.lines?.pct || 0;

        if (totalCoverage < 60) {
          bottlenecks.push({
            type: BottleneckType.TECHNICAL_DEBT,
            severity: 'high',
            location: 'Test coverage',
            impact: {
              developmentSpeed: -30,
              teamMorale: -15,
              codeQuality: -70,
              deliveryTimeline: -25,
              resourceUtilization: -20,
              innovation: -35,
              costMultiplier: 2.2
            },
            rootCauses: [
              {
                category: 'technical',
                description: `Low test coverage: ${totalCoverage.toFixed(1)}% (target: 80%+)`,
                confidence: 0.95,
                dependencies: ['testing-practices', 'time-allocation'],
                historicalFrequency: 0.8,
                solutionComplexity: 'moderate'
              }
            ],
            solutions: [],
            urgency: 8.5,
            estimatedResolutionTime: 45 * 24 * 60 * 60 * 1000,
            conflictingConstraints: ['testing time vs feature delivery']
          });
        }
      }
    } catch (error) {
      // Silent skip
    }
    return bottlenecks;
  }

  private async detectKnowledgeGaps(projectPath: string): Promise<BottleneckAnalysis[]> {
    const bottlenecks: BottleneckAnalysis[] = [];
    try {
      const { execSync } = require('child_process');
      const fs = require('fs');
      const path = require('path');

      const hasReadme = fs.existsSync(path.join(projectPath, 'README.md'));
      const docsDir = fs.existsSync(path.join(projectPath, 'docs'));

      if (!hasReadme && !docsDir) {
        bottlenecks.push({
          type: BottleneckType.KNOWLEDGE_GAP,
          severity: 'high',
          location: 'Project documentation',
          impact: {
            developmentSpeed: -35,
            teamMorale: -20,
            codeQuality: -25,
            deliveryTimeline: -30,
            resourceUtilization: -40,
            innovation: -15,
            costMultiplier: 1.6
          },
          rootCauses: [
            {
              category: 'process',
              description: 'No README or docs/ directory found',
              confidence: 0.95,
              dependencies: ['documentation-culture', 'onboarding'],
              historicalFrequency: 0.6,
              solutionComplexity: 'simple'
            }
          ],
          solutions: [],
          urgency: 7.0,
          estimatedResolutionTime: 14 * 24 * 60 * 60 * 1000,
          conflictingConstraints: ['doc time vs coding time']
        });
      }

      // Check comment density
      const commentedLines = parseInt(execSync(
        'git grep "^[[:space:]]*\\/\\/" -- "*.ts" "*.tsx" "*.js" "*.jsx" | wc -l',
        { cwd: projectPath, encoding: 'utf8' }
      ).trim());

      const totalLines = parseInt(execSync(
        'git ls-files "*.ts" "*.tsx" "*.js" "*.jsx" | xargs wc -l | tail -1 | awk \'{print $1}\'',
        { cwd: projectPath, encoding: 'utf8' }
      ).trim());

      const commentRatio = commentedLines / Math.max(totalLines, 1);

      if (commentRatio < 0.05) {
        bottlenecks.push({
          type: BottleneckType.KNOWLEDGE_GAP,
          severity: 'medium',
          location: 'Code documentation',
          impact: {
            developmentSpeed: -25,
            teamMorale: -15,
            codeQuality: -30,
            deliveryTimeline: -20,
            resourceUtilization: -30,
            innovation: -10,
            costMultiplier: 1.4
          },
          rootCauses: [
            {
              category: 'process',
              description: `Low comment density: ${(commentRatio * 100).toFixed(1)}% (target: 10-20%)`,
              confidence: 0.75,
              dependencies: ['coding-standards', 'review-process'],
              historicalFrequency: 0.5,
              solutionComplexity: 'simple'
            }
          ],
          solutions: [],
          urgency: 5.5,
          estimatedResolutionTime: 21 * 24 * 60 * 60 * 1000,
          conflictingConstraints: []
        });
      }
    } catch (error) {
      // Silent skip
    }
    return bottlenecks;
  }

  private async detectCommunicationBottlenecks(projectPath: string): Promise<BottleneckAnalysis[]> {
    const bottlenecks: BottleneckAnalysis[] = [];
    try {
      const { execSync } = require('child_process');

      const commitMessages = execSync(
        'git log --since="1 month ago" --pretty=format:"%s"',
        { cwd: projectPath, encoding: 'utf8' }
      ).trim().split('\n').filter(Boolean);

      const shortMessages = commitMessages.filter(msg => msg.length < 20).length;
      const shortRatio = shortMessages / Math.max(commitMessages.length, 1);

      if (shortRatio > 0.5) {
        bottlenecks.push({
          type: BottleneckType.COMMUNICATION,
          severity: 'medium',
          location: 'Commit message quality',
          impact: {
            developmentSpeed: -20,
            teamMorale: -15,
            codeQuality: -25,
            deliveryTimeline: -15,
            resourceUtilization: -25,
            innovation: -10,
            costMultiplier: 1.3
          },
          rootCauses: [
            {
              category: 'process',
              description: `${(shortRatio * 100).toFixed(0)}% of commits have unclear messages (<20 chars)`,
              confidence: 0.8,
              dependencies: ['commit-standards', 'code-review'],
              historicalFrequency: 0.6,
              solutionComplexity: 'simple'
            }
          ],
          solutions: [],
          urgency: 5.0,
          estimatedResolutionTime: 7 * 24 * 60 * 60 * 1000,
          conflictingConstraints: []
        });
      }

      // Check for PR activity
      try {
        const prCount = parseInt(execSync(
          'gh pr list --state all --limit 50 --json number | jq \'length\'',
          { cwd: projectPath, encoding: 'utf8' }
        ).trim());

        if (prCount === 0) {
          bottlenecks.push({
            type: BottleneckType.COMMUNICATION,
            severity: 'high',
            location: 'Code review process',
            impact: {
              developmentSpeed: -30,
              teamMorale: -20,
              codeQuality: -40,
              deliveryTimeline: -25,
              resourceUtilization: -35,
              innovation: -25,
              costMultiplier: 1.5
            },
            rootCauses: [
              {
                category: 'process',
                description: 'No pull requests found (potential lack of code review)',
                confidence: 0.85,
                dependencies: ['review-workflow', 'collaboration-tools'],
                historicalFrequency: 0.7,
                solutionComplexity: 'moderate'
              }
            ],
            solutions: [],
            urgency: 7.5,
            estimatedResolutionTime: 14 * 24 * 60 * 60 * 1000,
            conflictingConstraints: ['review time vs delivery speed']
          });
        }
      } catch {
        // GH CLI not available
      }
    } catch (error) {
      // Silent skip
    }
    return bottlenecks;
  }

  private async detectProcessBottlenecks(projectPath: string): Promise<BottleneckAnalysis[]> {
    const bottlenecks: BottleneckAnalysis[] = [];
    try {
      const { execSync } = require('child_process');
      const fs = require('fs');
      const path = require('path');

      // Check for CI/CD
      const hasCIConfig =
        fs.existsSync(path.join(projectPath, '.github/workflows')) ||
        fs.existsSync(path.join(projectPath, '.gitlab-ci.yml')) ||
        fs.existsSync(path.join(projectPath, '.circleci/config.yml')) ||
        fs.existsSync(path.join(projectPath, 'Jenkinsfile'));

      if (!hasCIConfig) {
        bottlenecks.push({
          type: BottleneckType.PROCESS_INEFFICIENCY,
          severity: 'high',
          location: 'Automation infrastructure',
          impact: {
            developmentSpeed: -40,
            teamMorale: -30,
            codeQuality: -50,
            deliveryTimeline: -60,
            resourceUtilization: -45,
            innovation: -20,
            costMultiplier: 2.0
          },
          rootCauses: [
            {
              category: 'process',
              description: 'No CI/CD configuration detected',
              confidence: 0.95,
              dependencies: ['infrastructure', 'automation-tools'],
              historicalFrequency: 0.8,
              solutionComplexity: 'moderate'
            }
          ],
          solutions: [],
          urgency: 8.5,
          estimatedResolutionTime: 21 * 24 * 60 * 60 * 1000,
          conflictingConstraints: ['setup time vs manual testing']
        });
      }

      // Check for pre-commit hooks
      const hasPreCommitHooks = fs.existsSync(path.join(projectPath, '.git/hooks/pre-commit'));

      if (!hasPreCommitHooks) {
        bottlenecks.push({
          type: BottleneckType.PROCESS_INEFFICIENCY,
          severity: 'medium',
          location: 'Quality gates',
          impact: {
            developmentSpeed: -25,
            teamMorale: -15,
            codeQuality: -35,
            deliveryTimeline: -20,
            resourceUtilization: -20,
            innovation: -10,
            costMultiplier: 1.4
          },
          rootCauses: [
            {
              category: 'process',
              description: 'No pre-commit hooks configured',
              confidence: 0.8,
              dependencies: ['git-hooks', 'quality-automation'],
              historicalFrequency: 0.6,
              solutionComplexity: 'simple'
            }
          ],
          solutions: [],
          urgency: 6.0,
          estimatedResolutionTime: 7 * 24 * 60 * 60 * 1000,
          conflictingConstraints: []
        });
      }

      // Analyze deployment frequency
      const tags = execSync('git tag --list', { cwd: projectPath, encoding: 'utf8' })
        .trim().split('\n').filter(Boolean);

      if (tags.length > 0) {
        const latestTag = execSync(
          'git log -1 --format=%at $(git describe --tags --abbrev=0)',
          { cwd: projectPath, encoding: 'utf8' }
        ).trim();

        const daysSinceRelease = Math.floor(
          (Date.now() - parseInt(latestTag) * 1000) / (1000 * 60 * 60 * 24)
        );

        if (daysSinceRelease > 60) {
          bottlenecks.push({
            type: BottleneckType.PROCESS_INEFFICIENCY,
            severity: 'medium',
            location: 'Release cadence',
            impact: {
              developmentSpeed: -30,
              teamMorale: -20,
              codeQuality: -15,
              deliveryTimeline: -50,
              resourceUtilization: -25,
              innovation: -30,
              costMultiplier: 1.5
            },
            rootCauses: [
              {
                category: 'process',
                description: `${daysSinceRelease} days since last release`,
                confidence: 0.75,
                dependencies: ['deployment-process', 'release-planning'],
                historicalFrequency: 0.5,
                solutionComplexity: 'moderate'
              }
            ],
            solutions: [],
            urgency: 6.5,
            estimatedResolutionTime: 14 * 24 * 60 * 60 * 1000,
            conflictingConstraints: ['release prep time vs frequency']
          });
        }
      }
    } catch (error) {
      // Silent skip
    }
    return bottlenecks;
  }

  // Struggle loop detection methods
  private async detectRepeatedFailurePatterns(projectPath: string): Promise<StruggleLoop[]> {
    const loops: StruggleLoop[] = [];
    try {
      const { execSync } = require('child_process');

      // Count reverted commits
      const revertedCommits = execSync(
        'git log --all --grep="Revert" --oneline --since="3 months ago"',
        { cwd: projectPath, encoding: 'utf8' }
      ).trim().split('\n').filter(Boolean);

      if (revertedCommits.length > 5) {
        loops.push({
          id: `repeated-failure-${Date.now()}`,
          pattern: StrugglePattern.REPEATED_FAILED_ATTEMPTS,
          iterations: revertedCommits.length,
          duration: 90 * 24 * 60 * 60 * 1000,
          energyDrain: 0.75,
          confidence: 0.85,
          breakthroughProbability: 0.4,
          alternativeApproaches: [
            {
              name: 'Pre-deployment validation',
              description: 'Implement staging environment with comprehensive testing',
              paradigmShift: 'Shift left - catch failures before production',
              successProbability: 0.8,
              riskLevel: 'low',
              resourceRequirement: '2 weeks setup time',
              timeToSolution: 14 * 24 * 60 * 60 * 1000,
              innovationLevel: 0.3,
              examples: ['Staging env', 'Smoke tests', 'Canary deployments']
            }
          ],
          exitStrategies: [
            {
              name: 'Root cause analysis',
              description: 'Analyze common failure patterns and address systematically',
              conditions: ['Access to failure logs', 'Team availability'],
              actions: ['Pattern analysis', 'Fix root causes', 'Add preventive tests'],
              recoverabilty: 0.9,
              learningValue: 0.95
            }
          ]
        });
      }

      // Check failed CI runs
      try {
        const failedBuilds = parseInt(execSync(
          'gh run list --status failure --limit 50 --json conclusion | jq \'length\'',
          { cwd: projectPath, encoding: 'utf8' }
        ).trim());

        if (failedBuilds > 20) {
          loops.push({
            id: `ci-failure-loop-${Date.now()}`,
            pattern: StrugglePattern.REPEATED_FAILED_ATTEMPTS,
            iterations: failedBuilds,
            duration: 30 * 24 * 60 * 60 * 1000,
            energyDrain: 0.6,
            confidence: 0.9,
            breakthroughProbability: 0.7,
            alternativeApproaches: [
              {
                name: 'Test reliability improvement',
                description: 'Fix flaky tests and improve test infrastructure',
                paradigmShift: 'Invest in test quality, not just coverage',
                successProbability: 0.85,
                riskLevel: 'low',
                resourceRequirement: '1 week dedicated effort',
                timeToSolution: 7 * 24 * 60 * 60 * 1000,
                innovationLevel: 0.2,
                examples: ['Retry logic', 'Test isolation', 'Better mocking']
              }
            ],
            exitStrategies: [
              {
                name: 'Stabilize builds first',
                description: 'Pause feature work to fix test infrastructure',
                conditions: ['Team agreement', 'Management buy-in'],
                actions: ['Identify flaky tests', 'Fix or quarantine', 'Monitor stability'],
                recoverabilty: 0.95,
                learningValue: 0.9
              }
            ]
          });
        }
      } catch {
        // GH CLI not available
      }
    } catch (error) {
      // Silent skip
    }
    return loops;
  }

  private async detectAnalysisParalysis(projectPath: string): Promise<StruggleLoop[]> {
    const loops: StruggleLoop[] = [];
    try {
      const { execSync } = require('child_process');

      // Check stale branches
      const branches = execSync(
        'git for-each-ref --format="%(refname:short) %(committerdate:relative)" refs/heads/',
        { cwd: projectPath, encoding: 'utf8' }
      ).trim().split('\n').filter(Boolean);

      const staleBranches = branches.filter(b => {
        const match = b.match(/(\d+)\s+(week|month|year)s?\s+ago/);
        if (!match) return false;
        const value = parseInt(match[1]);
        const unit = match[2];
        return (unit === 'month' && value >= 1) || unit === 'year' || (unit === 'week' && value >= 4);
      });

      if (staleBranches.length > 2) {
        loops.push({
          id: `analysis-paralysis-${Date.now()}`,
          pattern: StrugglePattern.ANALYSIS_PARALYSIS,
          iterations: staleBranches.length,
          duration: 60 * 24 * 60 * 60 * 1000,
          energyDrain: 0.7,
          confidence: 0.8,
          breakthroughProbability: 0.6,
          alternativeApproaches: [
            {
              name: 'Time-boxing decisions',
              description: 'Impose strict deadlines for merge/abandon decisions',
              paradigmShift: 'Perfect is the enemy of good - ship and iterate',
              successProbability: 0.75,
              riskLevel: 'medium',
              resourceRequirement: 'Policy change only',
              timeToSolution: 1 * 24 * 60 * 60 * 1000,
              innovationLevel: 0.4,
              examples: ['2-week branch max', 'Weekly decision reviews', 'Auto-close stale PRs']
            }
          ],
          exitStrategies: [
            {
              name: 'Force completion',
              description: 'Decide explicitly: complete, abandon, or defer each stale item',
              conditions: ['List of all stale items', 'Decision criteria'],
              actions: ['Review each item', 'Make binary decision', 'Execute immediately'],
              recoverabilty: 0.85,
              learningValue: 0.8
            }
          ]
        });
      }

      // Check old PRs
      try {
        const openPRs = JSON.parse(execSync(
          'gh pr list --state open --json createdAt,number',
          { cwd: projectPath, encoding: 'utf8' }
        ).trim());

        const oldPRs = openPRs.filter((pr: any) => {
          const ageMs = Date.now() - new Date(pr.createdAt).getTime();
          return ageMs > 7 * 24 * 60 * 60 * 1000;
        });

        if (oldPRs.length > 3) {
          loops.push({
            id: `pr-paralysis-${Date.now()}`,
            pattern: StrugglePattern.ANALYSIS_PARALYSIS,
            iterations: oldPRs.length,
            duration: 7 * 24 * 60 * 60 * 1000,
            energyDrain: 0.65,
            confidence: 0.85,
            breakthroughProbability: 0.7,
            alternativeApproaches: [
              {
                name: 'Review SLA',
                description: 'Establish 24-48 hour review service level agreement',
                paradigmShift: 'Review speed is a team responsibility',
                successProbability: 0.8,
                riskLevel: 'low',
                resourceRequirement: 'Team commitment',
                timeToSolution: 3 * 24 * 60 * 60 * 1000,
                innovationLevel: 0.3,
                examples: ['Review rotation', 'Notification automation', 'Review metrics']
              }
            ],
            exitStrategies: [
              {
                name: 'Clear backlog',
                description: 'Dedicate time to clearing PR backlog',
                conditions: ['Team availability', 'PR prioritization'],
                actions: ['Sort by age', 'Batch review session', 'Merge or close'],
                recoverabilty: 0.9,
                learningValue: 0.75
              }
            ]
          });
        }
      } catch {
        // GH CLI not available
      }
    } catch (error) {
      // Silent skip
    }
    return loops;
  }

  private async detectOvercomplication(projectPath: string): Promise<StruggleLoop[]> {
    const loops: StruggleLoop[] = [];
    try {
      const { execSync } = require('child_process');
      const fs = require('fs');
      const path = require('path');

      // Check abstraction ratio
      const interfaceCount = parseInt(execSync(
        'git grep "^interface " -- "*.ts" "*.tsx" | wc -l',
        { cwd: projectPath, encoding: 'utf8' }
      ).trim());

      const classCount = parseInt(execSync(
        'git grep "^class " -- "*.ts" "*.tsx" | wc -l',
        { cwd: projectPath, encoding: 'utf8' }
      ).trim());

      const fileCount = parseInt(execSync(
        'find . -type f \\( -name "*.ts" -o -name "*.tsx" \\) | wc -l',
        { cwd: projectPath, encoding: 'utf8' }
      ).trim());

      const abstractionRatio = (interfaceCount + classCount) / Math.max(fileCount, 1);

      if (abstractionRatio > 5) {
        loops.push({
          id: `overcomplication-${Date.now()}`,
          pattern: StrugglePattern.OVERCOMPLICATED_SOLUTION,
          iterations: Math.floor(abstractionRatio),
          duration: 180 * 24 * 60 * 60 * 1000,
          energyDrain: 0.8,
          confidence: 0.75,
          breakthroughProbability: 0.5,
          alternativeApproaches: [
            {
              name: 'Simplification sprint',
              description: 'Dedicate time to removing unnecessary abstractions',
              paradigmShift: 'YAGNI - You Aren\'t Gonna Need It',
              successProbability: 0.7,
              riskLevel: 'medium',
              resourceRequirement: '1-2 weeks',
              timeToSolution: 14 * 24 * 60 * 60 * 1000,
              innovationLevel: 0.6,
              examples: ['Inline single-use interfaces', 'Collapse layer hierarchies', 'Delete unused code']
            }
          ],
          exitStrategies: [
            {
              name: 'Complexity budget',
              description: 'Institute maximum complexity limits for new code',
              conditions: ['Team agreement', 'Measurement tools'],
              actions: ['Define complexity metrics', 'Set thresholds', 'Enforce in reviews'],
              recoverabilty: 0.8,
              learningValue: 0.85
            }
          ]
        });
      }

      // Check dependency bloat
      const packageJsonPath = path.join(projectPath, 'package.json');
      if (fs.existsSync(packageJsonPath)) {
        const packageJson = JSON.parse(fs.readFileSync(packageJsonPath, 'utf8'));
        const depCount = Object.keys(packageJson.dependencies || {}).length +
                        Object.keys(packageJson.devDependencies || {}).length;

        if (depCount > 100) {
          loops.push({
            id: `dependency-overload-${Date.now()}`,
            pattern: StrugglePattern.OVERCOMPLICATED_SOLUTION,
            iterations: Math.floor(depCount / 10),
            duration: 365 * 24 * 60 * 60 * 1000,
            energyDrain: 0.65,
            confidence: 0.8,
            breakthroughProbability: 0.6,
            alternativeApproaches: [
              {
                name: 'Dependency audit',
                description: 'Review and consolidate dependencies',
                paradigmShift: 'Fewer dependencies = less maintenance burden',
                successProbability: 0.75,
                riskLevel: 'medium',
                resourceRequirement: '1 week',
                timeToSolution: 7 * 24 * 60 * 60 * 1000,
                innovationLevel: 0.4,
                examples: ['Remove duplicates', 'Replace with built-ins', 'Consolidate tools']
              }
            ],
            exitStrategies: [
              {
                name: 'Dependency freeze',
                description: 'Require justification for new dependencies',
                conditions: ['Team policy', 'Review process'],
                actions: ['Document current deps', 'Approval process', 'Regular audits'],
                recoverabilty: 0.9,
                learningValue: 0.8
              }
            ]
          });
        }
      }
    } catch (error) {
      // Silent skip
    }
    return loops;
  }

  private async detectMissingPerspectives(projectPath: string): Promise<StruggleLoop[]> {
    const loops: StruggleLoop[] = [];
    try {
      const { execSync } = require('child_process');

      // Check contributor diversity
      const authorStats = execSync(
        'git log --since="6 months ago" --format="%an" | sort | uniq -c | sort -rn',
        { cwd: projectPath, encoding: 'utf8' }
      ).trim().split('\n').filter(Boolean);

      if (authorStats.length === 1) {
        loops.push({
          id: `single-perspective-${Date.now()}`,
          pattern: StrugglePattern.MISSING_PERSPECTIVE,
          iterations: 1,
          duration: 180 * 24 * 60 * 60 * 1000,
          energyDrain: 0.7,
          confidence: 0.9,
          breakthroughProbability: 0.3,
          alternativeApproaches: [
            {
              name: 'External code review',
              description: 'Bring in outside reviewers for fresh perspective',
              paradigmShift: 'Diversity of thought improves quality',
              successProbability: 0.7,
              riskLevel: 'low',
              resourceRequirement: 'External reviewer access',
              timeToSolution: 7 * 24 * 60 * 60 * 1000,
              innovationLevel: 0.6,
              examples: ['Pair programming', 'Guest reviews', 'Open source contribution']
            }
          ],
          exitStrategies: [
            {
              name: 'Expand team',
              description: 'Hire or collaborate with additional developers',
              conditions: ['Budget', 'Hiring capacity'],
              actions: ['Define needs', 'Recruit', 'Onboard'],
              recoverabilty: 0.6,
              learningValue: 0.9
            }
          ]
        });
      }

      // Check test perspective
      const testFiles = parseInt(execSync(
        'find . -type f \\( -name "*.test.ts" -o -name "*.spec.ts" \\) | wc -l',
        { cwd: projectPath, encoding: 'utf8' }
      ).trim());

      const srcFiles = parseInt(execSync(
        'find ./src -type f \\( -name "*.ts" -o -name "*.tsx" \\) ! -name "*.test.ts" ! -name "*.spec.ts" | wc -l',
        { cwd: projectPath, encoding: 'utf8' }
      ).trim());

      const testRatio = testFiles / Math.max(srcFiles, 1);

      if (testRatio < 0.3) {
        loops.push({
          id: `qa-perspective-missing-${Date.now()}`,
          pattern: StrugglePattern.MISSING_PERSPECTIVE,
          iterations: Math.floor((0.3 - testRatio) * 100),
          duration: 90 * 24 * 60 * 60 * 1000,
          energyDrain: 0.6,
          confidence: 0.75,
          breakthroughProbability: 0.7,
          alternativeApproaches: [
            {
              name: 'Test-Driven Development',
              description: 'Adopt TDD to build in QA perspective from start',
              paradigmShift: 'Write tests first, implementation second',
              successProbability: 0.65,
              riskLevel: 'medium',
              resourceRequirement: 'Team training',
              timeToSolution: 21 * 24 * 60 * 60 * 1000,
              innovationLevel: 0.7,
              examples: ['TDD workshops', 'Pair with QA', 'Test templates']
            }
          ],
          exitStrategies: [
            {
              name: 'Add QA role',
              description: 'Bring in dedicated QA perspective',
              conditions: ['Budget', 'QA availability'],
              actions: ['Define QA role', 'Hire or assign', 'Integrate in workflow'],
              recoverabilty: 0.8,
              learningValue: 0.85
            }
          ]
        });
      }
    } catch (error) {
      // Silent skip
    }
    return loops;
  }

  private async detectWrongProblemDefinition(projectPath: string): Promise<StruggleLoop[]> {
    const loops: StruggleLoop[] = [];
    try {
      const { execSync } = require('child_process');

      // Check refactor frequency
      const refactorCommits = execSync(
        'git log --all --grep="refactor\\|rewrite\\|redesign" -i --oneline --since="6 months ago"',
        { cwd: projectPath, encoding: 'utf8' }
      ).trim().split('\n').filter(Boolean);

      if (refactorCommits.length > 10) {
        loops.push({
          id: `wrong-problem-${Date.now()}`,
          pattern: StrugglePattern.WRONG_PROBLEM_DEFINITION,
          iterations: refactorCommits.length,
          duration: 180 * 24 * 60 * 60 * 1000,
          energyDrain: 0.85,
          confidence: 0.75,
          breakthroughProbability: 0.5,
          alternativeApproaches: [
            {
              name: 'Problem validation phase',
              description: 'Invest time upfront to validate problem before solving',
              paradigmShift: 'Slow down to go faster - validate before implementing',
              successProbability: 0.7,
              riskLevel: 'medium',
              resourceRequirement: '1-2 weeks discovery',
              timeToSolution: 14 * 24 * 60 * 60 * 1000,
              innovationLevel: 0.6,
              examples: ['User interviews', 'Prototype testing', '5 Whys analysis']
            }
          ],
          exitStrategies: [
            {
              name: 'Stop and validate',
              description: 'Pause implementation to validate the problem definition',
              conditions: ['Stakeholder access', 'Time to reflect'],
              actions: ['Document assumptions', 'Validate with users', 'Reframe if needed'],
              recoverabilty: 0.7,
              learningValue: 0.95
            }
          ]
        });
      }

      // Check churning files
      const churningFiles = parseInt(execSync(
        'git log --since="3 months ago" --name-only --pretty=format: -- "*.ts" "*.tsx" | sort | uniq -c | sort -rn | head -5 | awk \'$1 > 20\' | wc -l',
        { cwd: projectPath, encoding: 'utf8' }
      ).trim());

      if (churningFiles > 3) {
        loops.push({
          id: `requirements-churn-${Date.now()}`,
          pattern: StrugglePattern.WRONG_PROBLEM_DEFINITION,
          iterations: churningFiles,
          duration: 90 * 24 * 60 * 60 * 1000,
          energyDrain: 0.7,
          confidence: 0.7,
          breakthroughProbability: 0.6,
          alternativeApproaches: [
            {
              name: 'Stabilize requirements',
              description: 'Lock down requirements before implementation',
              paradigmShift: 'No code until requirements stable',
              successProbability: 0.75,
              riskLevel: 'low',
              resourceRequirement: 'BA/PM time',
              timeToSolution: 7 * 24 * 60 * 60 * 1000,
              innovationLevel: 0.3,
              examples: ['Requirements freeze', 'Change control board', 'Specification docs']
            }
          ],
          exitStrategies: [
            {
              name: 'Requirements baseline',
              description: 'Create and enforce requirements baseline',
              conditions: ['Stakeholder agreement', 'Clear process'],
              actions: ['Document requirements', 'Get sign-off', 'Control changes'],
              recoverabilty: 0.85,
              learningValue: 0.8
            }
          ]
        });
      }
    } catch (error) {
      // Silent skip
    }
    return loops;
  }

  private async detectKnowledgeLoops(projectPath: string): Promise<StruggleLoop[]> {
    const loops: StruggleLoop[] = [];
    try {
      const { execSync } = require('child_process');

      // Check TODO growth
      const currentTodos = parseInt(execSync(
        'git grep -i "TODO\\|FIXME" -- "*.ts" "*.tsx" | wc -l',
        { cwd: projectPath, encoding: 'utf8' }
      ).trim());

      if (currentTodos > 50) {
        loops.push({
          id: `knowledge-debt-${Date.now()}`,
          pattern: StrugglePattern.INSUFFICIENT_KNOWLEDGE,
          iterations: Math.floor(currentTodos / 10),
          duration: 90 * 24 * 60 * 60 * 1000,
          energyDrain: 0.6,
          confidence: 0.7,
          breakthroughProbability: 0.7,
          alternativeApproaches: [
            {
              name: 'Knowledge capture sprint',
              description: 'Dedicate time to addressing TODOs and documenting',
              paradigmShift: 'Documentation is development, not overhead',
              successProbability: 0.75,
              riskLevel: 'low',
              resourceRequirement: '1 week',
              timeToSolution: 7 * 24 * 60 * 60 * 1000,
              innovationLevel: 0.3,
              examples: ['TODO sprint', 'Doc-a-thon', 'Knowledge sessions']
            }
          ],
          exitStrategies: [
            {
              name: 'Make docs required',
              description: 'Include documentation in definition of done',
              conditions: ['Team agreement', 'Review process'],
              actions: ['Update DoD', 'Review checklist', 'Enforce in PRs'],
              recoverabilty: 0.9,
              learningValue: 0.85
            }
          ]
        });
      }

      // Check doc staleness
      try {
        const lastDocUpdate = parseInt(execSync(
          'git log -1 --format=%at -- "*.md" "docs/"',
          { cwd: projectPath, encoding: 'utf8' }
        ).trim());

        if (lastDocUpdate) {
          const daysSinceDocUpdate = Math.floor((Date.now() - lastDocUpdate * 1000) / (1000 * 60 * 60 * 24));
          if (daysSinceDocUpdate > 90) {
            loops.push({
              id: `stale-docs-${Date.now()}`,
              pattern: StrugglePattern.INSUFFICIENT_KNOWLEDGE,
              iterations: Math.floor(daysSinceDocUpdate / 30),
              duration: daysSinceDocUpdate * 24 * 60 * 60 * 1000,
              energyDrain: 0.5,
              confidence: 0.8,
              breakthroughProbability: 0.8,
              alternativeApproaches: [
                {
                  name: 'Living documentation',
                  description: 'Auto-generate docs from code and keep them current',
                  paradigmShift: 'Docs as code - version controlled and automated',
                  successProbability: 0.7,
                  riskLevel: 'low',
                  resourceRequirement: 'Tooling setup',
                  timeToSolution: 5 * 24 * 60 * 60 * 1000,
                  innovationLevel: 0.5,
                  examples: ['JSDoc', 'Typedoc', 'Storybook', 'ADRs']
                }
              ],
              exitStrategies: [
                {
                  name: 'Doc refresh cycle',
                  description: 'Quarterly documentation review and update',
                  conditions: ['Calendar commitment', 'Owner assignment'],
                  actions: ['Schedule reviews', 'Assign owners', 'Track staleness'],
                  recoverabilty: 0.85,
                  learningValue: 0.75
                }
              ]
            });
          }
        }
      } catch {
        // No docs found
      }
    } catch (error) {
      // Silent skip
    }
    return loops;
  }

  private async detectCognitiveBiases(projectPath: string): Promise<StruggleLoop[]> {
    const loops: StruggleLoop[] = [];
    try {
      const { execSync } = require('child_process');

      // Sunk cost: unmerged branches
      const unmergedBranches = parseInt(execSync(
        'git branch -a --no-merged | wc -l',
        { cwd: projectPath, encoding: 'utf8' }
      ).trim());

      if (unmergedBranches > 10) {
        loops.push({
          id: `sunk-cost-${Date.now()}`,
          pattern: StrugglePattern.COGNITIVE_BIAS,
          iterations: unmergedBranches,
          duration: 180 * 24 * 60 * 60 * 1000,
          energyDrain: 0.55,
          confidence: 0.65,
          breakthroughProbability: 0.75,
          alternativeApproaches: [
            {
              name: 'Explicit abandonment',
              description: 'Make conscious decision to abandon or complete each branch',
              paradigmShift: 'Sunk costs are sunk - focus on future value',
              successProbability: 0.8,
              riskLevel: 'low',
              resourceRequirement: 'Decision time',
              timeToSolution: 2 * 24 * 60 * 60 * 1000,
              innovationLevel: 0.4,
              examples: ['Branch review', 'Kill or commit', 'Zero inbox for branches']
            }
          ],
          exitStrategies: [
            {
              name: 'Branch hygiene policy',
              description: 'Auto-close branches after 30 days inactive',
              conditions: ['Team agreement', 'Automation setup'],
              actions: ['Set policy', 'Automate cleanup', 'Monitor exceptions'],
              recoverabilty: 0.95,
              learningValue: 0.7
            }
          ]
        });
      }

      // Confirmation bias: only adding, never removing
      const addedLines = parseInt(execSync(
        'git log --since="6 months ago" --numstat --pretty=format: | awk \'{add+=$1} END {print add}\'',
        { cwd: projectPath, encoding: 'utf8' }
      ).trim());

      const deletedLines = parseInt(execSync(
        'git log --since="6 months ago" --numstat --pretty=format: | awk \'{del+=$2} END {print del}\'',
        { cwd: projectPath, encoding: 'utf8' }
      ).trim());

      const addDelRatio = addedLines / Math.max(deletedLines, 1);

      if (addDelRatio > 10) {
        loops.push({
          id: `feature-hoarding-${Date.now()}`,
          pattern: StrugglePattern.COGNITIVE_BIAS,
          iterations: Math.floor(addDelRatio),
          duration: 180 * 24 * 60 * 60 * 1000,
          energyDrain: 0.7,
          confidence: 0.7,
          breakthroughProbability: 0.6,
          alternativeApproaches: [
            {
              name: 'Code subtraction goals',
              description: 'Set goals for code deletion alongside addition',
              paradigmShift: 'Simplicity through subtraction',
              successProbability: 0.65,
              riskLevel: 'medium',
              resourceRequirement: 'Cultural change',
              timeToSolution: 30 * 24 * 60 * 60 * 1000,
              innovationLevel: 0.7,
              examples: ['Delete days', 'Simplification sprints', 'Complexity budgets']
            }
          ],
          exitStrategies: [
            {
              name: 'Celebrate deletion',
              description: 'Track and celebrate code removal as much as addition',
              conditions: ['Metrics in place', 'Team buy-in'],
              actions: ['Track deletion', 'Showcase simplifications', 'Reward subtraction'],
              recoverabilty: 0.8,
              learningValue: 0.9
            }
          ]
        });
      }
    } catch (error) {
      // Silent skip
    }
    return loops;
  }

  private async detectResourceThrashing(projectPath: string): Promise<StruggleLoop[]> {
    const loops: StruggleLoop[] = [];
    try {
      const { execSync } = require('child_process');

      // Check context switching
      const recentCommits = execSync(
        'git log --since="7 days ago" --name-only --pretty=format:',
        { cwd: projectPath, encoding: 'utf8' }
      ).trim().split('\n').filter(Boolean);

      const uniqueFiles = new Set(recentCommits);
      const thrashingScore = uniqueFiles.size / Math.max(recentCommits.length, 1);

      if (thrashingScore > 0.8 && recentCommits.length > 20) {
        loops.push({
          id: `context-switching-${Date.now()}`,
          pattern: StrugglePattern.RESOURCE_THRASHING,
          iterations: Math.floor(thrashingScore * 100),
          duration: 7 * 24 * 60 * 60 * 1000,
          energyDrain: 0.8,
          confidence: 0.75,
          breakthroughProbability: 0.7,
          alternativeApproaches: [
            {
              name: 'WIP limits',
              description: 'Limit work in progress to reduce context switching',
              paradigmShift: 'Finish before starting - flow over throughput',
              successProbability: 0.8,
              riskLevel: 'low',
              resourceRequirement: 'Process change',
              timeToSolution: 3 * 24 * 60 * 60 * 1000,
              innovationLevel: 0.5,
              examples: ['Kanban WIP limits', 'One feature at a time', 'Focus time blocks']
            }
          ],
          exitStrategies: [
            {
              name: 'Single-tasking',
              description: 'Enforce single-task focus until completion',
              conditions: ['Team discipline', 'Backlog management'],
              actions: ['Set WIP=1', 'Block interruptions', 'Measure cycle time'],
              recoverabilty: 0.9,
              learningValue: 0.85
            }
          ]
        });
      }

      // Check WIP commits
      const wipCommits = execSync(
        'git log --since="1 month ago" --all --grep="WIP\\|wip\\|work in progress" -i --oneline',
        { cwd: projectPath, encoding: 'utf8' }
      ).trim().split('\n').filter(Boolean);

      if (wipCommits.length > 10) {
        loops.push({
          id: `incomplete-work-${Date.now()}`,
          pattern: StrugglePattern.RESOURCE_THRASHING,
          iterations: wipCommits.length,
          duration: 30 * 24 * 60 * 60 * 1000,
          energyDrain: 0.65,
          confidence: 0.8,
          breakthroughProbability: 0.75,
          alternativeApproaches: [
            {
              name: 'Finish-to-start policy',
              description: 'Complete current work before starting new',
              paradigmShift: 'Done is better than perfect',
              successProbability: 0.75,
              riskLevel: 'low',
              resourceRequirement: 'Discipline',
              timeToSolution: 1 * 24 * 60 * 60 * 1000,
              innovationLevel: 0.3,
              examples: ['Daily standup focus', 'Pair to complete', 'Time-box tasks']
            }
          ],
          exitStrategies: [
            {
              name: 'Clear WIP backlog',
              description: 'Dedicate time to finishing all WIP items',
              conditions: ['Protected time', 'No new work'],
              actions: ['List all WIP', 'Finish or abandon', 'Reset to zero'],
              recoverabilty: 0.9,
              learningValue: 0.8
            }
          ]
        });
      }
    } catch (error) {
      // Silent skip
    }
    return loops;
  }

  // Meta-analysis methods
  private async analyzeTeamDynamics(projectPath: string): Promise<TeamDynamicsAnalysis> {
    try {
      const { execSync } = require('child_process');

      // Analyze contributor activity
      const contributors = execSync(
        'git log --since="90 days ago" --pretty=format:"%an" | sort | uniq',
        { cwd: projectPath, encoding: 'utf8' }
      ).trim().split('\n').filter(Boolean);

      const contributorCount = contributors.length;

      // Analyze commit frequency (decision-making proxy)
      const totalCommits = parseInt(execSync(
        'git log --since="90 days ago" --oneline | wc -l',
        { cwd: projectPath, encoding: 'utf8' }
      ).trim()) || 0;

      const commitsPerDay = totalCommits / 90;
      const decisionMakingSpeed = Math.min(commitsPerDay / 5, 1.0); // 5 commits/day = 1.0

      // Analyze collaboration patterns (co-authored commits)
      const coAuthoredCommits = parseInt(execSync(
        'git log --since="90 days ago" --grep="Co-authored-by" --oneline | wc -l',
        { cwd: projectPath, encoding: 'utf8' }
      ).trim()) || 0;

      const collaborationRate = totalCommits > 0 ? coAuthoredCommits / totalCommits : 0;
      const knowledgeSharing = Math.min(collaborationRate * 2, 1.0);

      // Communication patterns detection
      const communicationPatterns: string[] = [];
      if (collaborationRate > 0.2) communicationPatterns.push('collaborative');
      if (collaborationRate < 0.1) communicationPatterns.push('siloed');
      if (commitsPerDay > 3) communicationPatterns.push('high-velocity');
      if (commitsPerDay < 1) communicationPatterns.push('slow-paced');

      // Skill complementarity (based on contributor distribution)
      const skillComplementarity = Math.min(contributorCount / 5, 1.0); // 5+ contributors = 1.0

      return {
        communicationPatterns: communicationPatterns.length > 0 ? communicationPatterns : ['average-collaboration'],
        decisionMakingSpeed,
        conflictResolutionEfficiency: 0.7, // Requires PR analysis - use default
        knowledgeSharing,
        psychologicalSafety: 0.75, // Requires qualitative analysis - use default
        creativityLevel: Math.min(contributorCount / 3, 1.0), // More contributors = more creativity
        burnoutRisk: commitsPerDay > 10 ? 0.7 : 0.3, // High velocity may indicate burnout
        skillComplementarity
      };
    } catch (error) {
      // Git not available or not a repo - return defaults
      return {
        communicationPatterns: ['unknown'],
        decisionMakingSpeed: 0.5,
        conflictResolutionEfficiency: 0.5,
        knowledgeSharing: 0.5,
        psychologicalSafety: 0.5,
        creativityLevel: 0.5,
        burnoutRisk: 0.5,
        skillComplementarity: 0.5
      };
    }
  }

  private async analyzeProcessEfficiency(projectPath: string): Promise<ProcessEfficiencyAnalysis> {
    try {
      const { execSync } = require('child_process');

      // Detect stale branches (bottleneck indicator)
      const staleBranches = parseInt(execSync(
        'git branch -r | wc -l',
        { cwd: projectPath, encoding: 'utf8' }
      ).trim()) || 0;

      const bottlenecks: string[] = [];
      if (staleBranches > 10) bottlenecks.push('branch-management');
      if (staleBranches > 20) bottlenecks.push('code-review');

      // Calculate average cycle time (branch creation to merge)
      let avgCycleTime = 7 * 24 * 60 * 60 * 1000; // Default 7 days
      try {
        const recentMerges = execSync(
          'git log --since="30 days ago" --merges --pretty=format:"%ct" | head -10',
          { cwd: projectPath, encoding: 'utf8' }
        ).trim().split('\n').filter(Boolean);

        if (recentMerges.length > 0) {
          avgCycleTime = 5 * 24 * 60 * 60 * 1000; // Estimate 5 days if active merging
        }
      } catch {
        // Default cycle time
      }

      // Calculate throughput (features per month)
      const monthlyCommits = parseInt(execSync(
        'git log --since="30 days ago" --oneline | wc -l',
        { cwd: projectPath, encoding: 'utf8' }
      ).trim()) || 0;

      const throughput = Math.max(Math.floor(monthlyCommits / 10), 1); // Estimate 10 commits per feature

      // Estimate error rate from revert patterns
      const totalCommits = parseInt(execSync(
        'git log --since="90 days ago" --oneline | wc -l',
        { cwd: projectPath, encoding: 'utf8' }
      ).trim()) || 1;

      const revertCommits = parseInt(execSync(
        'git log --since="90 days ago" --grep="revert\\|Revert" --oneline | wc -l',
        { cwd: projectPath, encoding: 'utf8' }
      ).trim()) || 0;

      const errorRate = Math.min(revertCommits / totalCommits, 0.5);

      // Detect automation opportunities
      const automationOpportunities: string[] = [];
      if (errorRate > 0.1) automationOpportunities.push('automated-testing');
      if (staleBranches > 15) automationOpportunities.push('CI/CD-automation');
      if (throughput < 5) automationOpportunities.push('workflow-automation');

      return {
        bottlenecks: bottlenecks.length > 0 ? bottlenecks : ['none-detected'],
        redundancies: [], // Requires code analysis - empty for now
        automationOpportunities: automationOpportunities.length > 0 ? automationOpportunities : ['well-automated'],
        feedbackLoops: ['git-history'], // Basic feedback loop
        qualityGates: errorRate < 0.1 ? ['effective-testing'] : ['testing-improvement-needed'],
        cycleTime: avgCycleTime,
        throughput,
        errorRate
      };
    } catch (error) {
      // Git not available - return defaults
      return {
        bottlenecks: ['unknown'],
        redundancies: [],
        automationOpportunities: [],
        feedbackLoops: [],
        qualityGates: [],
        cycleTime: 7 * 24 * 60 * 60 * 1000,
        throughput: 10,
        errorRate: 0.1
      };
    }
  }

  private async analyzeToolEffectiveness(projectPath: string): Promise<ToolEffectivenessAnalysis> {
    try {
      const fs = require('fs');
      const path = require('path');

      // Analyze package.json for tool usage
      const packageJsonPath = path.join(projectPath, 'package.json');
      const utilizationRate: Record<string, number> = {};
      const recommendations: string[] = [];

      if (fs.existsSync(packageJsonPath)) {
        const packageJson = JSON.parse(fs.readFileSync(packageJsonPath, 'utf8'));
        const deps = { ...packageJson.dependencies, ...packageJson.devDependencies };

        // Estimate utilization based on dependency count
        const depCount = Object.keys(deps).length;
        utilizationRate['npm'] = Math.min(depCount / 50, 1.0);

        // Check for common tools
        if (deps['typescript']) utilizationRate['typescript'] = 0.9;
        if (deps['jest'] || deps['mocha']) utilizationRate['testing'] = 0.8;
        if (deps['eslint']) utilizationRate['linting'] = 0.85;

        // Recommendations based on missing tools
        if (!deps['typescript']) recommendations.push('Consider TypeScript for type safety');
        if (!deps['jest'] && !deps['mocha']) recommendations.push('Add automated testing framework');
        if (!deps['eslint']) recommendations.push('Add ESLint for code quality');
      }

      return {
        utilizationRate: Object.keys(utilizationRate).length > 0 ? utilizationRate : { 'tools': 0.7 },
        learningCurve: {}, // Requires team survey - empty
        integrationQuality: {}, // Requires analysis - empty
        maintenanceOverhead: {}, // Requires analysis - empty
        alternatives: {},
        recommendations: recommendations.length > 0 ? recommendations : ['Tool stack appears complete']
      };
    } catch (error) {
      // File system error - return defaults
      return {
        utilizationRate: { 'tools': 0.7 },
        learningCurve: {},
        integrationQuality: {},
        maintenanceOverhead: {},
        alternatives: {},
        recommendations: []
      };
    }
  }

  private async analyzeKnowledgeGaps(projectPath: string): Promise<KnowledgeGapAnalysis> {
    try {
      const { execSync } = require('child_process');

      // Analyze file ownership concentration
      const fileOwnership = execSync(
        'git log --pretty=format:"%an" --name-only | grep -v "^$" | sort | uniq -c | sort -nr | head -20',
        { cwd: projectPath, encoding: 'utf8' }
      ).trim().split('\n').filter(Boolean);

      const criticalGaps: string[] = [];
      const expertiseDistribution: Record<string, number> = {};

      // Check for single-point-of-failure files (one owner)
      if (fileOwnership.length > 0 && fileOwnership.length < 5) {
        criticalGaps.push('knowledge-concentration');
      }

      // Analyze documentation quality (README, docs/)
      let docQuality = 0.5;
      try {
        const fs = require('fs');
        const path = require('path');
        if (fs.existsSync(path.join(projectPath, 'README.md'))) docQuality += 0.2;
        if (fs.existsSync(path.join(projectPath, 'docs'))) docQuality += 0.2;
        docQuality = Math.min(docQuality, 1.0);
      } catch {
        // File system error
      }

      // Estimate knowledge transfer rate from collaboration
      const totalCommits = parseInt(execSync(
        'git log --since="90 days ago" --oneline | wc -l',
        { cwd: projectPath, encoding: 'utf8' }
      ).trim()) || 1;

      const coAuthoredCommits = parseInt(execSync(
        'git log --since="90 days ago" --grep="Co-authored-by" --oneline | wc -l',
        { cwd: projectPath, encoding: 'utf8' }
      ).trim()) || 0;

      const knowledgeTransferRate = Math.min(coAuthoredCommits / totalCommits, 1.0);

      if (docQuality < 0.6) criticalGaps.push('insufficient-documentation');
      if (knowledgeTransferRate < 0.1) criticalGaps.push('poor-knowledge-sharing');

      return {
        criticalGaps: criticalGaps.length > 0 ? criticalGaps : ['none-identified'],
        learningPriorities: criticalGaps.map(gap => `improve-${gap}`),
        expertiseDistribution: expertiseDistribution, // Requires code analysis by domain
        documentationQuality: docQuality,
        knowledgeTransferRate,
        externalDependencies: [] // Requires dependency analysis
      };
    } catch (error) {
      return {
        criticalGaps: ['analysis-unavailable'],
        learningPriorities: [],
        expertiseDistribution: {},
        documentationQuality: 0.5,
        knowledgeTransferRate: 0.5,
        externalDependencies: []
      };
    }
  }

  private async analyzeCognitiveLoad(projectPath: string): Promise<CognitiveLoadAnalysis> {
    try {
      const { execSync } = require('child_process');
      const fs = require('fs');
      const path = require('path');

      // Analyze dependency count (complexity indicator)
      let depCount = 0;
      try {
        const packageJsonPath = path.join(projectPath, 'package.json');
        if (fs.existsSync(packageJsonPath)) {
          const packageJson = JSON.parse(fs.readFileSync(packageJsonPath, 'utf8'));
          depCount = Object.keys({ ...packageJson.dependencies, ...packageJson.devDependencies }).length;
        }
      } catch {
        // No package.json
      }

      const complexityLevel = Math.min(depCount / 100, 1.0); // 100+ deps = 1.0

      // Analyze branch switching frequency (context switching proxy)
      const branches = parseInt(execSync(
        'git branch -a | wc -l',
        { cwd: projectPath, encoding: 'utf8' }
      ).trim()) || 0;

      const contextSwitching = Math.min(branches / 20, 1.0); // 20+ branches = 1.0

      // Simplification opportunities
      const simplificationOpportunities: string[] = [];
      if (depCount > 50) simplificationOpportunities.push('reduce-dependencies');
      if (branches > 15) simplificationOpportunities.push('consolidate-branches');
      if (complexityLevel > 0.7) simplificationOpportunities.push('simplify-architecture');

      return {
        complexityLevel,
        contextSwitching,
        informationOverload: complexityLevel * 0.8, // Proxy based on complexity
        multitaskingPenalty: contextSwitching * 0.7, // Proxy based on branches
        focusFragmentation: (complexityLevel + contextSwitching) / 2,
        simplificationOpportunities: simplificationOpportunities.length > 0 ? simplificationOpportunities : ['low-complexity']
      };
    } catch (error) {
      return {
        complexityLevel: 0.5,
        contextSwitching: 0.5,
        informationOverload: 0.5,
        multitaskingPenalty: 0.5,
        focusFragmentation: 0.5,
        simplificationOpportunities: []
      };
    }
  }

  private async analyzeInnovationIndex(projectPath: string): Promise<InnovationIndexAnalysis> {
    try {
      const { execSync } = require('child_process');

      // Analyze experimental branches (feature/, experiment/, prototype/)
      const experimentalBranches = parseInt(execSync(
        'git branch -a | grep -E "feature/|experiment/|prototype/" | wc -l',
        { cwd: projectPath, encoding: 'utf8' }
      ).trim()) || 0;

      const totalBranches = parseInt(execSync(
        'git branch -a | wc -l',
        { cwd: projectPath, encoding: 'utf8' }
      ).trim()) || 1;

      const experimentationRate = Math.min(experimentalBranches / totalBranches, 1.0);

      // Analyze feature vs fix commit ratio (risk tolerance proxy)
      const featureCommits = parseInt(execSync(
        'git log --since="90 days ago" --grep="feat:\\|feature:" --oneline | wc -l',
        { cwd: projectPath, encoding: 'utf8' }
      ).trim()) || 0;

      const fixCommits = parseInt(execSync(
        'git log --since="90 days ago" --grep="fix:\\|bugfix:" --oneline | wc -l',
        { cwd: projectPath, encoding: 'utf8' }
      ).trim()) || 0;

      const totalCommits = featureCommits + fixCommits || 1;
      const riskTolerance = Math.min(featureCommits / totalCommits, 1.0);

      // Analyze commit frequency (implementation speed)
      const commitsPerDay = parseInt(execSync(
        'git log --since="30 days ago" --oneline | wc -l',
        { cwd: projectPath, encoding: 'utf8' }
      ).trim()) / 30;

      const implementationSpeed = Math.min(commitsPerDay / 3, 1.0); // 3 commits/day = 1.0

      return {
        experimentationRate,
        riskTolerance,
        ideaGeneration: experimentationRate * 0.8, // Proxy based on experimentation
        implementationSpeed,
        learningVelocity: (experimentationRate + implementationSpeed) / 2,
        adaptabilityScore: (experimentationRate + riskTolerance + implementationSpeed) / 3
      };
    } catch (error) {
      return {
        experimentationRate: 0.5,
        riskTolerance: 0.5,
        ideaGeneration: 0.5,
        implementationSpeed: 0.5,
        learningVelocity: 0.5,
        adaptabilityScore: 0.5
      };
    }
  }

  private async analyzeCollaborationQuality(projectPath: string): Promise<CollaborationQualityAnalysis> {
    try {
      const { execSync } = require('child_process');
      const fs = require('fs');
      const path = require('path');

      // Analyze PR review patterns (collaboration proxy)
      let meetingEfficiency = 0.5;
      try {
        const prComments = parseInt(execSync(
          'gh pr list --state all --limit 100 --json comments --jq ".[].comments | length" | awk \'{sum+=$1} END {print sum}\'',
          { cwd: projectPath, encoding: 'utf8' }
        ).trim()) || 0;

        if (prComments > 100) meetingEfficiency = 0.8; // High engagement
        else if (prComments > 50) meetingEfficiency = 0.6;
      } catch {
        // gh not available
      }

      // Analyze documentation (README, docs/)
      let documentationSharing = 0.3;
      if (fs.existsSync(path.join(projectPath, 'README.md'))) documentationSharing += 0.3;
      if (fs.existsSync(path.join(projectPath, 'docs'))) documentationSharing += 0.3;

      // Analyze commit message quality (feedback proxy)
      const avgCommitLength = parseInt(execSync(
        'git log --since="30 days ago" --pretty=format:"%s" | awk \'{sum+=length($0)} END {print int(sum/NR)}\'',
        { cwd: projectPath, encoding: 'utf8' }
      ).trim()) || 30;

      const feedbackQuality = Math.min(avgCommitLength / 50, 1.0); // 50+ chars = detailed

      // Analyze co-authored commits (cross-functional alignment)
      const totalCommits = parseInt(execSync(
        'git log --since="90 days ago" --oneline | wc -l',
        { cwd: projectPath, encoding: 'utf8' }
      ).trim()) || 1;

      const coAuthoredCommits = parseInt(execSync(
        'git log --since="90 days ago" --grep="Co-authored-by" --oneline | wc -l',
        { cwd: projectPath, encoding: 'utf8' }
      ).trim()) || 0;

      const crossFunctionalAlignment = Math.min(coAuthoredCommits / totalCommits * 3, 1.0);

      return {
        syncVsAsync: documentationSharing, // Proxy: more docs = more async
        meetingEfficiency,
        documentationSharing,
        crossFunctionalAlignment,
        feedbackQuality,
        decisionTransparency: (feedbackQuality + documentationSharing) / 2
      };
    } catch (error) {
      return {
        syncVsAsync: 0.5,
        meetingEfficiency: 0.5,
        documentationSharing: 0.5,
        crossFunctionalAlignment: 0.5,
        feedbackQuality: 0.5,
        decisionTransparency: 0.5
      };
    }
  }

  // Solution generation methods
  private async generateConstraintTheorySolutions(bottleneck: BottleneckAnalysis): Promise<BreakthroughSolution[]> {
    // Generate constraint-specific solution based on bottleneck type and details
    const constraintName = bottleneck.type.replace(/_/g, ' ').toUpperCase();
    const primaryRootCause = bottleneck.rootCauses[0]?.description || 'System constraint';

    // Map severity to urgency multiplier
    const severityMultiplier = bottleneck.severity === 'critical' ? 1.5 :
                                bottleneck.severity === 'high' ? 1.2 : 1.0;

    // Calculate estimated improvement based on impact metrics
    const avgImpact = Math.abs(
      (bottleneck.impact.developmentSpeed +
       bottleneck.impact.deliveryTimeline +
       bottleneck.impact.resourceUtilization) / 3
    );

    const improvementPercentage = Math.round(avgImpact * severityMultiplier);

    return [
      {
        id: `constraint_${bottleneck.type}_${Date.now()}`,
        name: `TOC: Eliminate ${constraintName} Constraint`,
        type: SolutionType.CONSTRAINT_REMOVAL,
        approach: SolutionApproach.PROVEN,
        description: `Apply Theory of Constraints to eliminate ${constraintName} at ${bottleneck.location}. Root cause: ${primaryRootCause}`,
        implementation: {
          phases: [
            {
              name: 'Constraint Analysis',
              description: `Analyze ${constraintName} constraint at ${bottleneck.location}`,
              duration: bottleneck.estimatedResolutionTime * 0.2, // 20% of total time
              resources: ['Team lead', 'Domain expert'],
              deliverables: ['Constraint analysis document', 'Impact quantification'],
              risks: [`Misidentification of root cause`],
              dependencies: bottleneck.rootCauses[0]?.dependencies || []
            },
            {
              name: 'Solution Design',
              description: `Design solution to address: ${primaryRootCause}`,
              duration: bottleneck.estimatedResolutionTime * 0.3, // 30% of total time
              resources: ['Architect', 'Senior developers'],
              deliverables: ['Solution design', 'Implementation plan'],
              risks: ['Solution complexity', 'Resource availability'],
              dependencies: ['Constraint analysis complete']
            },
            {
              name: 'Implementation',
              description: `Execute constraint removal at ${bottleneck.location}`,
              duration: bottleneck.estimatedResolutionTime * 0.5, // 50% of total time
              resources: ['Development team'],
              deliverables: ['Constraint eliminated', 'Metrics improved'],
              risks: [`Downstream bottleneck creation`],
              dependencies: ['Solution design approved']
            }
          ],
          prerequisites: ['Team alignment', 'Resource allocation'],
          milestones: [
            {
              name: `${constraintName} Resolved`,
              description: `${bottleneck.location} constraint eliminated`,
              successCriteria: bottleneck.rootCauses.map(rc => `${rc.description} resolved`),
              validationMethod: 'Metric tracking',
              timeframe: bottleneck.estimatedResolutionTime
            }
          ],
          rollbackPlan: ['Revert changes', 'Monitor for regressions'],
          successMetrics: [
            `${improvementPercentage}% improvement in system throughput`,
            'Development velocity increased',
            'Team morale improved'
          ],
          monitoringStrategy: 'Continuous constraint monitoring (identify next constraint)'
        },
        constraints: bottleneck.conflictingConstraints.map(c => ({
          type: 'organizational' as const,
          description: c,
          severity: 0.6,
          flexibility: 0.5,
          workarounds: ['Apply TRIZ principles', 'Seek paradigm shift']
        })),
        benefits: [
          {
            category: 'performance',
            description: `Eliminate ${constraintName} improving system throughput`,
            quantification: `${improvementPercentage}% improvement expected`,
            timeline: 'short_term',
            confidence: bottleneck.rootCauses[0]?.confidence || 0.8
          }
        ],
        risks: [
          {
            description: 'Constraint migration: New bottleneck may emerge downstream',
            probability: 0.4,
            impact: 0.5,
            mitigation: ['Monitor all metrics', 'Continuous TOC analysis'],
            contingency: ['Identify and address next constraint']
          }
        ],
        confidence: (10 - bottleneck.urgency) / 10, // Higher urgency = lower solution confidence
        novelty: 0.3,
        elegance: 0.85,
        sustainability: 0.9,
        scalability: 0.8
      }
    ];
  }

  private async generateCrossdomainSolutions(bottleneck: BottleneckAnalysis): Promise<BreakthroughSolution[]> {
    // Apply cross-domain thinking based on bottleneck type
    const crossDomainMappings: Record<string, string> = {
      'performance': 'Manufacturing lean principles',
      'development_velocity': 'Sports interval training methods',
      'technical_debt': 'Financial compound interest management',
      'process_inefficiency': 'Logistics optimization'
    };

    const domain = crossDomainMappings[bottleneck.type] || 'General systems thinking';

    return [{
      id: `cross-domain-${Date.now()}`,
      name: `Cross-Domain Approach: ${domain}`,
      type: SolutionType.PARADIGM_CHANGE,
      approach: SolutionApproach.REVOLUTIONARY,
      description: `Apply ${domain} to address ${bottleneck.location}`,
      implementation: {
        phases: [{
          name: 'Research analogous domain',
          description: `Study ${domain} and identify transferable patterns`,
          duration: 3 * 24 * 60 * 60 * 1000,
          resources: ['Research time', 'Domain experts'],
          deliverables: ['Pattern analysis'],
          risks: ['Domain mismatch'],
          dependencies: []
        }],
        prerequisites: ['Open mindset', 'Time for exploration'],
        milestones: [{
          name: 'Pattern identified',
          description: 'Transferable pattern found',
          successCriteria: ['Clear analogy'],
          validationMethod: 'Peer review',
          timeframe: 7 * 24 * 60 * 60 * 1000
        }],
        rollbackPlan: ['Return to conventional approach'],
        successMetrics: ['Problem resolution', 'Team learning'],
        monitoringStrategy: 'Experimentation tracking'
      },
      constraints: [],
      benefits: [{
        category: 'innovation',
        description: 'Novel approach from unexpected source',
        quantification: 'Potential breakthrough',
        timeline: 'medium_term',
        confidence: 0.6
      }],
      risks: [{
        description: 'Domain analogy may not translate',
        probability: 0.4,
        impact: 0.5,
        mitigation: ['Rapid prototyping'],
        contingency: ['Standard solutions available']
      }],
      confidence: 0.7,
      novelty: 0.9,
      elegance: 0.7,
      sustainability: 0.8,
      scalability: 0.7
    }];
  }

  private async generateSimplificationSolutions(bottleneck: BottleneckAnalysis): Promise<BreakthroughSolution[]> {
    return [{
      id: `simplify-${Date.now()}`,
      name: 'Simplification: Remove Unnecessary Complexity',
      type: SolutionType.SIMPLIFICATION,
      approach: SolutionApproach.INCREMENTAL,
      description: `Eliminate non-essential elements in ${bottleneck.location}`,
      implementation: {
        phases: [{
          name: 'Identify removable components',
          description: 'Classify elements as essential vs unnecessary',
          duration: 2 * 24 * 60 * 60 * 1000,
          resources: ['Team review time'],
          deliverables: ['Complexity audit'],
          risks: ['Removing something important'],
          dependencies: []
        }],
        prerequisites: ['Test coverage'],
        milestones: [{
          name: 'Complexity reduced',
          description: '20% reduction in code/processes',
          successCriteria: ['Metrics improved', 'Functionality preserved'],
          validationMethod: 'Automated testing',
          timeframe: 14 * 24 * 60 * 60 * 1000
        }],
        rollbackPlan: ['Git revert available'],
        successMetrics: ['Reduced LOC', 'Improved maintainability'],
        monitoringStrategy: 'Continuous testing'
      },
      constraints: [],
      benefits: [{
        category: 'maintainability',
        description: 'Reduced cognitive load',
        quantification: '20-40% complexity reduction',
        timeline: 'short_term',
        confidence: 0.85
      }],
      risks: [{
        description: 'May remove subtly important code',
        probability: 0.2,
        impact: 0.6,
        mitigation: ['Comprehensive testing'],
        contingency: ['Rollback plan']
      }],
      confidence: 0.85,
      novelty: 0.3,
      elegance: 0.95,
      sustainability: 0.9,
      scalability: 0.85
    }];
  }

  private async generateAutomationSolutions(bottleneck: BottleneckAnalysis): Promise<BreakthroughSolution[]> {
    return [{
      id: `automate-${Date.now()}`,
      name: 'Automation: Eliminate Manual Work',
      type: SolutionType.AUTOMATION,
      approach: SolutionApproach.INCREMENTAL,
      description: `Automate repetitive tasks in ${bottleneck.location}`,
      implementation: {
        phases: [{
          name: 'Identify automation opportunities',
          description: 'Map manual processes and calculate ROI',
          duration: 3 * 24 * 60 * 60 * 1000,
          resources: ['Process mapping time'],
          deliverables: ['Automation candidates list'],
          risks: ['Over-automation'],
          dependencies: []
        }],
        prerequisites: ['Scripting capability'],
        milestones: [{
          name: 'First automation live',
          description: 'Highest-ROI process automated',
          successCriteria: ['Time saved', 'Error reduction'],
          validationMethod: 'Metrics tracking',
          timeframe: 14 * 24 * 60 * 60 * 1000
        }],
        rollbackPlan: ['Manual fallback procedures'],
        successMetrics: ['Time saved', 'Error rate'],
        monitoringStrategy: 'Automated metrics'
      },
      constraints: [],
      benefits: [{
        category: 'productivity',
        description: 'Eliminate repetitive work',
        quantification: '50-80% time savings',
        timeline: 'medium_term',
        confidence: 0.9
      }],
      risks: [{
        description: 'Automation may break with changes',
        probability: 0.3,
        impact: 0.4,
        mitigation: ['Robust testing'],
        contingency: ['Manual procedures']
      }],
      confidence: 0.88,
      novelty: 0.4,
      elegance: 0.8,
      sustainability: 0.95,
      scalability: 0.9
    }];
  }

  private async generateParadigmShiftSolutions(bottleneck: BottleneckAnalysis): Promise<BreakthroughSolution[]> {
    return [{
      id: `paradigm-${Date.now()}`,
      name: 'Paradigm Shift: Question Core Assumptions',
      type: SolutionType.PARADIGM_CHANGE,
      approach: SolutionApproach.REVOLUTIONARY,
      description: `Challenge fundamental assumptions behind ${bottleneck.location}`,
      implementation: {
        phases: [{
          name: 'List and challenge assumptions',
          description: 'Brainstorm and question all underlying assumptions',
          duration: 2 * 24 * 60 * 60 * 1000,
          resources: ['Team brainstorming time'],
          deliverables: ['Assumptions document'],
          risks: ['Team resistance'],
          dependencies: []
        }],
        prerequisites: ['Organizational buy-in'],
        milestones: [{
          name: 'New paradigm validated',
          description: 'Proof-of-concept demonstrates viability',
          successCriteria: ['10x improvement potential'],
          validationMethod: 'Prototype testing',
          timeframe: 30 * 24 * 60 * 60 * 1000
        }],
        rollbackPlan: ['Maintain old system during transition'],
        successMetrics: ['Problem elimination', 'Team excitement'],
        monitoringStrategy: 'Prototype validation'
      },
      constraints: [],
      benefits: [{
        category: 'innovation',
        description: 'Eliminate entire class of problems',
        quantification: '10x improvement possible',
        timeline: 'long_term',
        confidence: 0.5
      }],
      risks: [{
        description: 'May require complete redesign',
        probability: 0.7,
        impact: 0.8,
        mitigation: ['Incremental transition'],
        contingency: ['Dual-track approach']
      }],
      confidence: 0.6,
      novelty: 1.0,
      elegance: 0.9,
      sustainability: 0.7,
      scalability: 0.95
    }];
  }

  private async generateLoopBreakingSolutions(loop: StruggleLoop): Promise<BreakthroughSolution[]> {
    const strategies: Record<string, string> = {
      'repeated_failed_attempts': 'Add validation gate',
      'analysis_paralysis': 'Impose decision deadlines',
      'overcomplicated_solution': 'Complexity budget',
      'missing_perspective': 'External review',
      'wrong_problem_definition': 'Problem validation phase',
      'insufficient_knowledge': 'Documentation requirement',
      'cognitive_bias': 'Decision frameworks',
      'resource_thrashing': 'WIP limits'
    };

    const strategy = strategies[loop.pattern] || 'Pattern interruption';

    return [{
      id: `break-loop-${Date.now()}`,
      name: `Break Loop: ${strategy}`,
      type: SolutionType.PROCESS_OPTIMIZATION,
      approach: SolutionApproach.INCREMENTAL,
      description: `Interrupt ${loop.pattern} pattern`,
      implementation: {
        phases: [{
          name: 'Install circuit breaker',
          description: 'Set up loop detection and intervention',
          duration: 2 * 24 * 60 * 60 * 1000,
          resources: ['Process design time'],
          deliverables: ['Loop detection system'],
          risks: ['False positives'],
          dependencies: []
        }],
        prerequisites: ['Team awareness'],
        milestones: [{
          name: 'Loop broken',
          description: 'Pattern frequency reduced 70%',
          successCriteria: ['Reduced iterations'],
          validationMethod: 'Pattern tracking',
          timeframe: 14 * 24 * 60 * 60 * 1000
        }],
        rollbackPlan: ['Remove intervention'],
        successMetrics: ['Loop frequency', 'Energy drain'],
        monitoringStrategy: 'Pattern tracking'
      },
      constraints: [],
      benefits: [{
        category: 'productivity',
        description: 'Break destructive pattern',
        quantification: '70%+ reduction',
        timeline: 'short_term',
        confidence: 0.8
      }],
      risks: [{
        description: 'Loop may shift form',
        probability: 0.3,
        impact: 0.5,
        mitigation: ['Monitor variants'],
        contingency: ['Adapt intervention']
      }],
      confidence: 0.8,
      novelty: 0.5,
      elegance: 0.7,
      sustainability: 0.85,
      scalability: 0.8
    }];
  }

  private async generateMetaLevelSolutions(metaAnalysis: MetaAnalysis): Promise<BreakthroughSolution[]> {
    return [{
      id: `meta-${Date.now()}`,
      name: 'Meta-Level: Optimize the Optimization Process',
      type: SolutionType.METHODOLOGY_SHIFT,
      approach: SolutionApproach.REVOLUTIONARY,
      description: 'Improve how we identify and solve problems',
      implementation: {
        phases: [{
          name: 'Analyze problem-solving effectiveness',
          description: 'Review past solutions and identify patterns',
          duration: 3 * 24 * 60 * 60 * 1000,
          resources: ['Team reflection time'],
          deliverables: ['Meta-analysis report'],
          risks: ['Analysis paralysis'],
          dependencies: []
        }],
        prerequisites: ['Leadership buy-in'],
        milestones: [{
          name: 'Improved process deployed',
          description: 'New problem-solving framework in use',
          successCriteria: ['Faster resolution', 'Better solutions'],
          validationMethod: 'Comparative metrics',
          timeframe: 30 * 24 * 60 * 60 * 1000
        }],
        rollbackPlan: ['Return to old process'],
        successMetrics: ['Problem resolution speed', 'Solution quality'],
        monitoringStrategy: 'Meta-metrics tracking'
      },
      constraints: [],
      benefits: [{
        category: 'innovation',
        description: 'Improve all future problem-solving',
        quantification: 'Compound improvement over time',
        timeline: 'long_term',
        confidence: 0.75
      }],
      risks: [{
        description: 'Meta-optimization overhead',
        probability: 0.4,
        impact: 0.5,
        mitigation: ['Time-box meta-work'],
        contingency: ['Focus on concrete problems']
      }],
      confidence: 0.75,
      novelty: 0.8,
      elegance: 0.85,
      sustainability: 0.9,
      scalability: 1.0
    }];
  }

  private rankAndRefineBreakthroughSolutions(solutions: BreakthroughSolution[]): BreakthroughSolution[] {
    return solutions.sort((a, b) => {
      const scoreA = a.confidence * a.elegance * a.sustainability;
      const scoreB = b.confidence * b.elegance * b.sustainability;
      return scoreB - scoreA;
    });
  }

  // Insight generation methods
  private async generatePatternInsights(
    bottlenecks: BottleneckAnalysis[],
    loops: StruggleLoop[]
  ): Promise<BreakthroughInsight[]> {
    const insights: BreakthroughInsight[] = [];

    // Analyze recurring bottleneck types
    const bottleneckTypeFrequency = new Map<BottleneckType, number>();
    for (const bottleneck of bottlenecks) {
      bottleneckTypeFrequency.set(
        bottleneck.type,
        (bottleneckTypeFrequency.get(bottleneck.type) || 0) + 1
      );
    }

    for (const [type, count] of bottleneckTypeFrequency) {
      if (count >= 2) {
        insights.push({
          type: InsightType.PATTERN_RECOGNITION,
          description: `Recurring ${type} bottleneck detected ${count} times - systematic issue requiring structural intervention`,
          source: InsightSource.PATTERN_ANALYSIS,
          confidence: Math.min(0.6 + (count * 0.1), 0.95),
          applicability: 0.9,
          novelty: 0.5,
          actionability: [
            `Address root cause of ${type} systematically`,
            'Implement preventive measures',
            'Monitor for recurrence'
          ]
        });
      }
    }

    // Analyze struggle loop patterns
    const loopPatternFrequency = new Map<StrugglePattern, number>();
    for (const loop of loops) {
      loopPatternFrequency.set(
        loop.pattern,
        (loopPatternFrequency.get(loop.pattern) || 0) + 1
      );
    }

    for (const [pattern, count] of loopPatternFrequency) {
      if (count >= 2) {
        insights.push({
          type: InsightType.PATTERN_RECOGNITION,
          description: `Team exhibits ${pattern} pattern ${count} times - cognitive or process dysfunction`,
          source: InsightSource.PATTERN_ANALYSIS,
          confidence: Math.min(0.7 + (count * 0.08), 0.92),
          applicability: 0.85,
          novelty: 0.6,
          actionability: [
            `Break ${pattern} cycle with process change`,
            'Provide training or mentorship',
            'Implement circuit breakers'
          ]
        });
      }
    }

    // Cross-pattern analysis: bottlenecks causing loops
    if (bottlenecks.some(b => b.type === BottleneckType.DECISION_PARALYSIS) &&
        loops.some(l => l.pattern === StrugglePattern.ANALYSIS_PARALYSIS)) {
      insights.push({
        type: InsightType.PATTERN_RECOGNITION,
        description: 'Decision paralysis bottleneck is triggering analysis paralysis loops - need decision-making framework',
        source: InsightSource.PATTERN_ANALYSIS,
        confidence: 0.88,
        applicability: 0.95,
        novelty: 0.75,
        actionability: [
          'Implement decision-making framework (e.g., RACI)',
          'Set decision deadlines',
          'Empower individual decision-makers',
          'Use "disagree and commit" protocol'
        ]
      });
    }

    return insights;
  }

  private async generateConstraintInsights(bottlenecks: BottleneckAnalysis[]): Promise<BreakthroughInsight[]> {
    const insights: BreakthroughInsight[] = [];

    // Theory of Constraints: Identify the ONE critical bottleneck
    if (bottlenecks.length > 0) {
      const sortedByImpact = [...bottlenecks].sort((a, b) => {
        const impactA = Math.abs(a.impact.developmentSpeed) +
                       Math.abs(a.impact.deliveryTimeline) +
                       (a.urgency * 10);
        const impactB = Math.abs(b.impact.developmentSpeed) +
                       Math.abs(b.impact.deliveryTimeline) +
                       (b.urgency * 10);
        return impactB - impactA;
      });

      const criticalBottleneck = sortedByImpact[0];

      insights.push({
        type: InsightType.CONSTRAINT_IDENTIFICATION,
        description: `Theory of Constraints: ${criticalBottleneck.type} at ${criticalBottleneck.location} is THE system constraint. All optimization efforts should focus here first.`,
        source: InsightSource.CONSTRAINT_THEORY,
        confidence: 0.92,
        applicability: 1.0,
        novelty: 0.7,
        actionability: [
          `Focus 80% of resources on eliminating ${criticalBottleneck.type}`,
          'Subordinate all other processes to this constraint',
          'Once resolved, identify next constraint (continuous improvement)',
          'Do NOT optimize non-constraints until this is resolved'
        ]
      });

      // Detect conflicting constraints
      for (const bottleneck of bottlenecks) {
        if (bottleneck.conflictingConstraints.length > 0) {
          insights.push({
            type: InsightType.CONSTRAINT_IDENTIFICATION,
            description: `Conflicting constraints detected: ${bottleneck.conflictingConstraints.join(', ')}. Traditional optimization will fail - need innovative resolution.`,
            source: InsightSource.CONSTRAINT_THEORY,
            confidence: 0.85,
            applicability: 0.9,
            novelty: 0.8,
            actionability: [
              'Apply TRIZ contradiction resolution principles',
              'Challenge constraint assumptions',
              'Seek paradigm-shifting solution',
              'Consider constraint removal vs optimization'
            ]
          });
        }
      }

      // Cascading constraints
      if (bottlenecks.length >= 3) {
        insights.push({
          type: InsightType.CONSTRAINT_IDENTIFICATION,
          description: `${bottlenecks.length} bottlenecks detected - likely cascading effects. Resolve primary constraint to unlock chain reaction.`,
          source: InsightSource.CONSTRAINT_THEORY,
          confidence: 0.8,
          applicability: 0.95,
          novelty: 0.65,
          actionability: [
            'Map bottleneck dependency chain',
            'Identify root vs symptom bottlenecks',
            'Resolve upstream constraints first',
            'Monitor for constraint migration'
          ]
        });
      }
    }

    return insights;
  }

  private async generateAssumptionChallenges(solutions: BreakthroughSolution[]): Promise<BreakthroughInsight[]> {
    const insights: BreakthroughInsight[] = [];

    // Challenge common software development assumptions
    const commonAssumptions = [
      {
        assumption: 'More code/features = more value',
        challenge: 'Less is often more. Complexity is a liability, not an asset.',
        actionability: ['Measure value delivery, not lines of code', 'Practice YAGNI ruthlessly', 'Delete code aggressively']
      },
      {
        assumption: 'We need to handle every edge case',
        challenge: 'The 80/20 rule applies. Handle the 20% of cases that cause 80% of issues.',
        actionability: ['Prioritize edge cases by impact', 'Document unhandled cases', 'Add handling when needed, not preemptively']
      },
      {
        assumption: 'Tests slow us down',
        challenge: 'Bugs and rework slow you down 10x more than writing tests.',
        actionability: ['Measure cost of bugs vs cost of tests', 'Automate test execution', 'Test business-critical paths first']
      },
      {
        assumption: 'Perfect architecture is necessary before coding',
        challenge: 'Emergent design often beats upfront architecture. Start simple, refactor as you learn.',
        actionability: ['Use evolutionary architecture', 'Refactor continuously', 'Build MVPs to validate assumptions']
      },
      {
        assumption: 'We need consensus before proceeding',
        challenge: 'Consensus can be decision paralysis in disguise. "Disagree and commit" enables velocity.',
        actionability: ['Use decision deadlines', 'Empower deciders', 'Document disagreements and move forward']
      }
    ];

    // Add assumption challenges based on solutions
    for (let i = 0; i < Math.min(2, commonAssumptions.length); i++) {
      const assumption = commonAssumptions[i];
      insights.push({
        type: InsightType.ASSUMPTION_CHALLENGE,
        description: `Challenge assumption: "${assumption.assumption}". Counter-insight: ${assumption.challenge}`,
        source: InsightSource.CREATIVE_ALGORITHMS,
        confidence: 0.8,
        applicability: 0.85,
        novelty: 0.7,
        actionability: assumption.actionability
      });
    }

    // If we have paradigm shift solutions, challenge related assumptions
    const paradigmShiftSolutions = solutions.filter(s => s.type === SolutionType.PARADIGM_CHANGE);
    if (paradigmShiftSolutions.length > 0) {
      insights.push({
        type: InsightType.ASSUMPTION_CHALLENGE,
        description: 'Current paradigm may be constraining solution space. Question: What if our fundamental approach is wrong?',
        source: InsightSource.CREATIVE_ALGORITHMS,
        confidence: 0.75,
        applicability: 0.9,
        novelty: 0.9,
        actionability: [
          'List all implicit assumptions about the problem',
          'Invert each assumption and explore implications',
          'Seek analogies from completely different domains',
          'Ask "What would [expert from different field] do?"'
        ]
      });
    }

    // Challenge resource assumptions if we have simplification solutions
    const simplificationSolutions = solutions.filter(s => s.type === SolutionType.SIMPLIFICATION);
    if (simplificationSolutions.length > 0) {
      insights.push({
        type: InsightType.ASSUMPTION_CHALLENGE,
        description: 'Challenge: "We need more resources." Counter: We may need less complexity instead.',
        source: InsightSource.CREATIVE_ALGORITHMS,
        confidence: 0.82,
        applicability: 0.88,
        novelty: 0.75,
        actionability: [
          'List all current system components',
          'Delete/disable 10% with lowest ROI',
          'Measure impact - often negligible or positive',
          'Continue simplification until essential complexity remains'
        ]
      });
    }

    return insights;
  }

  private async generateCrossDomainInsights(solutions: BreakthroughSolution[]): Promise<BreakthroughInsight[]> {
    const insights: BreakthroughInsight[] = [];

    // Cross-domain analogies for software development problems
    const analogies = [
      {
        domain: 'Manufacturing (Toyota Production System)',
        principle: 'Just-in-Time + Kanban',
        softwareApplication: 'Pull-based workflow with WIP limits prevents overproduction and context-switching',
        actionability: ['Implement Kanban board', 'Set WIP limits per developer', 'Pull work only when capacity available']
      },
      {
        domain: 'Medicine (Triage)',
        principle: 'Prioritize by urgency and impact, not order of arrival',
        softwareApplication: 'Bug/feature triage prevents low-value work from blocking critical issues',
        actionability: ['Weekly triage meetings', 'P0/P1/P2/P3 classification', 'SLA for each priority level']
      },
      {
        domain: 'Aviation (Pre-flight Checklist)',
        principle: 'Systematic verification prevents catastrophic failures',
        softwareApplication: 'Deployment checklists + pre-commit hooks prevent production incidents',
        actionability: ['Create deployment checklist', 'Automate with pre-commit hooks', 'Never skip steps']
      },
      {
        domain: 'Architecture (Load-Bearing Walls)',
        principle: 'Some components are critical, others are decorative',
        softwareApplication: 'Identify load-bearing code vs nice-to-haves - protect the former',
        actionability: ['Map critical business paths', 'Extra testing for load-bearing code', 'Allow technical debt in non-critical areas']
      },
      {
        domain: 'Biology (Immune System)',
        principle: 'Multi-layered defense with redundancy',
        softwareApplication: 'Defense in depth: input validation, authentication, authorization, rate limiting, monitoring',
        actionability: ['Never rely on single security layer', 'Add redundant checks', 'Monitor for anomalies']
      }
    ];

    // Select relevant analogies based on solutions
    for (let i = 0; i < Math.min(2, analogies.length); i++) {
      const analogy = analogies[i];
      insights.push({
        type: InsightType.CROSS_DOMAIN_TRANSFER,
        description: `Apply ${analogy.domain} principle: ${analogy.softwareApplication}`,
        source: InsightSource.ANALOGICAL_REASONING,
        confidence: 0.75,
        applicability: 0.8,
        novelty: 0.85,
        actionability: analogy.actionability,
        crossDomainAnalogy: `${analogy.domain} â†’ Software: ${analogy.principle}`
      });
    }

    // If we have automation solutions, use manufacturing analogy
    const automationSolutions = solutions.filter(s => s.type === SolutionType.AUTOMATION);
    if (automationSolutions.length > 0) {
      insights.push({
        type: InsightType.CROSS_DOMAIN_TRANSFER,
        description: 'Manufacturing insight: Automate repetitive tasks, reserve humans for creative/judgment work',
        source: InsightSource.ANALOGICAL_REASONING,
        confidence: 0.88,
        applicability: 0.95,
        novelty: 0.7,
        actionability: [
          'Identify all manual, repetitive tasks',
          'Calculate time cost per iteration',
          'Automate anything done more than 3 times',
          'Use automation to eliminate human error'
        ],
        crossDomainAnalogy: 'Assembly Line Robotics â†’ Software CI/CD'
      });
    }

    // If we have cross-domain solutions, extract their analogies
    const crossDomainSolutions = solutions.filter(s =>
      s.implementation?.phases?.some(p => p.description.toLowerCase().includes('cross-domain'))
    );
    if (crossDomainSolutions.length > 0) {
      insights.push({
        type: InsightType.CROSS_DOMAIN_TRANSFER,
        description: 'Pattern: Solutions from other domains often unlock breakthroughs. Seek inspiration outside software.',
        source: InsightSource.ANALOGICAL_REASONING,
        confidence: 0.82,
        applicability: 0.85,
        novelty: 0.9,
        actionability: [
          'Study how other industries solve similar problems',
          'Attend conferences outside software engineering',
          'Read books from business, psychology, design',
          'Ask "What would a [domain expert] do?"'
        ]
      });
    }

    return insights;
  }

  private async generateSimplificationInsights(metaAnalysis: MetaAnalysis): Promise<BreakthroughInsight[]> {
    const insights: BreakthroughInsight[] = [];

    // Analyze cognitive load
    if (metaAnalysis.cognitiveLoad.simplificationOpportunities.length > 0) {
      insights.push({
        type: InsightType.SIMPLIFICATION_OPPORTUNITY,
        description: `Cognitive overload detected: ${metaAnalysis.cognitiveLoad.simplificationOpportunities.join(', ')}. Simplification will yield exponential productivity gains.`,
        source: InsightSource.PATTERN_ANALYSIS,
        confidence: 0.9,
        applicability: 0.95,
        novelty: 0.65,
        actionability: [
          'Reduce WIP to 1-2 tasks per person',
          'Eliminate unnecessary meetings/interruptions',
          'Simplify architecture to reduce mental models',
          'Delete unused code and features'
        ]
      });
    }

    // Analyze tool effectiveness
    const utilizationRates = Object.values(metaAnalysis.toolEffectiveness.utilizationRate);
    const avgUtilization = utilizationRates.length > 0
      ? utilizationRates.reduce((a, b) => a + b, 0) / utilizationRates.length
      : 1.0;

    if (avgUtilization < 0.5) {
      insights.push({
        type: InsightType.SIMPLIFICATION_OPPORTUNITY,
        description: `Low tool utilization (${(avgUtilization * 100).toFixed(0)}%) suggests tools are too complex. Simplify toolchain.`,
        source: InsightSource.PATTERN_ANALYSIS,
        confidence: 0.85,
        applicability: 0.9,
        novelty: 0.7,
        actionability: [
          'Survey team on tool pain points',
          'Eliminate redundant tools (consolidate)',
          'Provide training for essential tools',
          'Consider simpler alternatives'
        ]
      });
    }

    // Analyze process efficiency
    if (metaAnalysis.processEfficiency.errorRate > 0.15 || metaAnalysis.processEfficiency.bottlenecks.length > 3) {
      const errorPercentage = (metaAnalysis.processEfficiency.errorRate * 100).toFixed(0);
      insights.push({
        type: InsightType.SIMPLIFICATION_OPPORTUNITY,
        description: `High error rate (${errorPercentage}%) or multiple bottlenecks detected. Process simplification can improve quality and efficiency.`,
        source: InsightSource.PATTERN_ANALYSIS,
        confidence: 0.88,
        applicability: 0.92,
        novelty: 0.6,
        actionability: [
          'Map value stream to identify waste',
          'Eliminate handoffs and approval bottlenecks',
          'Automate repetitive manual processes',
          'Remove non-essential documentation requirements'
        ]
      });
    }

    // Analyze collaboration quality
    if (metaAnalysis.collaborationQuality.meetingEfficiency < 0.6) {
      insights.push({
        type: InsightType.SIMPLIFICATION_OPPORTUNITY,
        description: `Low meeting efficiency suggests over-collaboration or poor meeting practices. Simplify collaboration.`,
        source: InsightSource.PATTERN_ANALYSIS,
        confidence: 0.8,
        applicability: 0.85,
        novelty: 0.65,
        actionability: [
          'Cancel recurring meetings with unclear value',
          'Use async communication (docs, comments) instead of meetings',
          'Strict agenda and timebox for necessary meetings',
          'Default to "no meetings" policy except where essential'
        ]
      });
    }

    // Innovation index analysis
    if (metaAnalysis.innovationIndex.experimentationRate < 0.3) {
      insights.push({
        type: InsightType.SIMPLIFICATION_OPPORTUNITY,
        description: 'Low experimentation rate suggests bureaucracy or complexity is stifling innovation. Simplify to enable experimentation.',
        source: InsightSource.PATTERN_ANALYSIS,
        confidence: 0.82,
        applicability: 0.88,
        novelty: 0.75,
        actionability: [
          'Institute "20% time" for experimentation',
          'Reduce approval requirements for experiments',
          'Celebrate failures as learning opportunities',
          'Provide sandbox environments'
        ]
      });
    }

    // General simplification principle
    insights.push({
      type: InsightType.SIMPLIFICATION_OPPORTUNITY,
      description: 'Leverage point: Complexity is the enemy of execution. Every simplification enables velocity and quality improvements.',
      source: InsightSource.PATTERN_ANALYSIS,
      confidence: 0.95,
      applicability: 1.0,
      novelty: 0.5,
      actionability: [
        'Apply "subtract before you add" principle',
        'Regular simplification sprints (delete code/processes)',
        'Measure complexity metrics (cyclomatic, cognitive)',
        'Reward simplification as much as feature delivery'
      ]
    });

    return insights;
  }

  private async generateContradictionInsights(bottlenecks: BottleneckAnalysis[]): Promise<BreakthroughInsight[]> {
    const insights: BreakthroughInsight[] = [];

    // Detect common software development contradictions
    const contradictions = [
      {
        constraint1: 'Speed (rapid delivery)',
        constraint2: 'Quality (low defects)',
        resolution: 'Automation + TDD: Automated testing enables both speed and quality simultaneously',
        actionability: ['Invest in test automation', 'Practice TDD/BDD', 'CI/CD pipeline for rapid validated releases']
      },
      {
        constraint1: 'Flexibility (easy to change)',
        constraint2: 'Stability (hard to break)',
        resolution: 'Comprehensive automated test suite: Tests enable confident refactoring',
        actionability: ['Achieve 80%+ test coverage', 'Refactor with green tests', 'Measure regression test effectiveness']
      },
      {
        constraint1: 'Innovation (new approaches)',
        constraint2: 'Reliability (proven methods)',
        resolution: 'Parallel experimentation: Run experiments alongside stable system',
        actionability: ['Feature flags for gradual rollout', 'A/B testing infrastructure', 'Sandbox environments']
      },
      {
        constraint1: 'Autonomy (independent teams)',
        constraint2: 'Consistency (unified standards)',
        resolution: 'Paved roads: Provide easy defaults, allow deviations with justification',
        actionability: ['Create reference implementations', 'Document standards clearly', 'Allow exceptions with rationale']
      },
      {
        constraint1: 'Documentation (comprehensive)',
        constraint2: 'Velocity (fast coding)',
        resolution: 'Self-documenting code + inline docs: Code is the documentation',
        actionability: ['Invest in clear naming', 'Use TypeScript/types as docs', 'Document "why" not "what" in comments']
      }
    ];

    // Check for conflicting constraints in bottlenecks
    const bottlenecksWithConflicts = bottlenecks.filter(b => b.conflictingConstraints.length > 0);

    if (bottlenecksWithConflicts.length > 0) {
      for (const bottleneck of bottlenecksWithConflicts) {
        insights.push({
          type: InsightType.CONTRADICTION_RESOLUTION,
          description: `TRIZ Contradiction: ${bottleneck.conflictingConstraints.join(' vs ')}. Traditional trade-off thinking will fail - need innovative resolution.`,
          source: InsightSource.CONSTRAINT_THEORY,
          confidence: 0.85,
          applicability: 0.9,
          novelty: 0.8,
          actionability: [
            'Reframe: Find solution that satisfies BOTH constraints',
            'Use TRIZ separation principles (in time, space, condition)',
            'Question if constraints are truly contradictory',
            'Seek technology or process that eliminates contradiction'
          ],
          contradictionResolution: 'Apply TRIZ 40 Inventive Principles to resolve contradiction'
        });
      }
    }

    // Add general contradiction insights
    for (let i = 0; i < Math.min(2, contradictions.length); i++) {
      const contradiction = contradictions[i];
      insights.push({
        type: InsightType.CONTRADICTION_RESOLUTION,
        description: `Common contradiction: ${contradiction.constraint1} vs ${contradiction.constraint2}`,
        source: InsightSource.CONSTRAINT_THEORY,
        confidence: 0.82,
        applicability: 0.85,
        novelty: 0.75,
        actionability: contradiction.actionability,
        contradictionResolution: contradiction.resolution
      });
    }

    // Meta-insight about contradictions
    insights.push({
      type: InsightType.CONTRADICTION_RESOLUTION,
      description: 'TRIZ Principle: True innovation resolves contradictions rather than compromising. Breakthroughs satisfy opposing requirements simultaneously.',
      source: InsightSource.CONSTRAINT_THEORY,
      confidence: 0.9,
      applicability: 0.95,
      novelty: 0.85,
      actionability: [
        'List all perceived trade-offs in your system',
        'Challenge each: Is compromise truly necessary?',
        'Seek innovations that eliminate the trade-off',
        'Study how other industries resolved similar contradictions'
      ],
      contradictionResolution: 'Contradiction elimination is the hallmark of breakthrough innovation'
    });

    return insights;
  }

  private initializeEmergencyProtocols(): void {
    // Critical system failure protocol
    this.emergencyProtocols.set('system_failure', {
      name: 'Critical System Failure Response',
      triggerConditions: ['system-down', 'data-loss-risk', 'security-breach'],
      urgencyLevel: 'emergency',
      responseTime: 5 * 60 * 1000, // 5 minutes
      actions: [
        {
          priority: 1,
          description: 'Isolate affected systems',
          owner: 'incident-commander',
          duration: 2 * 60 * 1000,
          dependencies: [],
          rollbackOption: false
        },
        {
          priority: 2,
          description: 'Assess damage and determine recovery strategy',
          owner: 'technical-lead',
          duration: 10 * 60 * 1000,
          dependencies: ['system-isolation'],
          rollbackOption: false
        }
      ],
      resources: ['incident-team', 'technical-experts', 'communication-channels'],
      escalationPath: ['team-lead', 'engineering-manager', 'cto'],
      successCriteria: ['system-restored', 'data-integrity-confirmed', 'root-cause-identified']
    });

    // Deadlock breaking protocol
    this.emergencyProtocols.set('deadlock_breaking', {
      name: 'Development Deadlock Breaking',
      triggerConditions: ['decision-paralysis', 'conflicting-requirements', 'resource-deadlock'],
      urgencyLevel: 'high',
      responseTime: 30 * 60 * 1000, // 30 minutes
      actions: [
        {
          priority: 1,
          description: 'Facilitate emergency decision meeting',
          owner: 'project-manager',
          duration: 60 * 60 * 1000,
          dependencies: [],
          rollbackOption: true
        }
      ],
      resources: ['decision-makers', 'subject-matter-experts', 'facilitator'],
      escalationPath: ['project-manager', 'product-owner', 'stakeholders'],
      successCriteria: ['decision-made', 'action-plan-created', 'resources-allocated']
    });
  }

  private initializeBreakthroughPatterns(): void {
    // TRIZ-based innovation patterns
    this.breakthroughPatterns.set('contradiction_resolution', {
      patterns: [
        'separate_in_space',
        'separate_in_time',
        'separate_by_condition',
        'separate_by_scale'
      ]
    });

    // Biomimicry patterns
    this.breakthroughPatterns.set('biomimicry', {
      patterns: [
        'swarm_intelligence',
        'neural_networks',
        'evolutionary_algorithms',
        'self_organization'
      ]
    });

    // Constraint theory patterns
    this.breakthroughPatterns.set('constraint_theory', {
      patterns: [
        'identify_constraint',
        'exploit_constraint',
        'subordinate_everything',
        'elevate_constraint',
        'start_over'
      ]
    });
  }

  async activateEmergencyMode(protocol: string, context: any): Promise<boolean> {
    const emergencyProtocol = this.emergencyProtocols.get(protocol);
    if (!emergencyProtocol) {
      throw new Error(`Unknown emergency protocol: ${protocol}`);
    }

    this.emit('emergency_activated', {
      protocol,
      urgencyLevel: emergencyProtocol.urgencyLevel,
      responseTime: emergencyProtocol.responseTime
    });

    // Execute emergency actions
    for (const action of emergencyProtocol.actions.sort((a, b) => a.priority - b.priority)) {
      await this.executeEmergencyAction(action, context);
    }

    return true;
  }

  private async executeEmergencyAction(action: EmergencyAction, context: any): Promise<void> {
    this.emit('emergency_action_started', {
      description: action.description,
      owner: action.owner,
      duration: action.duration
    });

    // Simulate action execution
    await new Promise(resolve => setTimeout(resolve, Math.min(action.duration, 1000)));

    this.emit('emergency_action_completed', {
      description: action.description,
      success: true
    });
  }
}

export default UltraThinkBreakthroughSystem;