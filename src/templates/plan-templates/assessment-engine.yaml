# Assessment Engine for Quality Auditing Template
# Based on Assessment Engine Phase 1 v6.6.0 (commit: 22c2ce2)
# Actual effort: 14 hours (estimated: 12 hours, 86% accuracy)

name: "Assessment Engine - Pattern Detection & Quality Auditing"
category: "Quality Assurance"
estimated_effort: "14 hours ± 2 hours"  # Updated with actual data
confidence: 90%  # Based on successful v6.6.0 Phase 1

phases:
  phase1_pattern_detection:
    effort: "6 hours"
    description: "Implement keyword-based pattern detection"
    tasks:
      - task: "Define pattern categories"
        effort: "2 hours"
        details: |
          Security: auth, login, jwt, password, token, crypto
          API: api, route, endpoint, controller, handler
          UI: component, jsx, tsx, vue, svelte, react
          Test: test, spec, .test., .spec., __tests__
          Database: migration, schema, database, sql, prisma
      - task: "Implement pattern matching"
        effort: "3 hours"
        details: |
          Keyword matching in claim context
          Multiple pattern categories per claim
          Priority assignment (critical, high, medium, low)
      - task: "Add pattern configuration"
        effort: "1 hour"
        details: |
          JSON configuration file
          71 keywords across 5 categories
          Extensible for new patterns
    code_examples:
      - file: "src/agents/verification/assessment-engine.ts"
        lines: "1-200"
        pattern: |
          class AssessmentEngine {
            needsAssessment(claim: Claim): boolean {
              const context = claim.context?.toLowerCase() || '';
              const securityPatterns = ['auth', 'login', 'jwt', 'password'];
              return securityPatterns.some(p => context.includes(p));
            }
          }
      - file: ".versatil/verification/assessment-config.json"
        lines: "1-100"
        pattern: |
          {
            "assessmentRules": {
              "security": {
                "patterns": ["auth", "login", "jwt", "password"],
                "priority": "critical"
              }
            }
          }
    lessons:
      - "✅ Keyword matching sufficient for Phase 1"
      - "✅ Security patterns = critical priority"
      - "⚠️ Pattern detection took longer than expected (false positives)"

  phase2_assessment_planning:
    effort: "5 hours"
    description: "Generate assessment plans for detected patterns"
    tasks:
      - task: "Define assessment tool integrations"
        effort: "2 hours"
        details: |
          Security: semgrep (0 vulnerabilities mandatory)
          Test coverage: jest (80%+ mandatory, 90%+ for security)
          Accessibility: axe-core (90%+ WCAG 2.1 AA mandatory)
          Performance: lighthouse (90+ score optional)
          API compliance: @redocly/cli (optional)
      - task: "Implement assessment planning logic"
        effort: "2 hours"
        details: |
          Match pattern → select appropriate tools
          Set thresholds (mandatory vs optional)
          Estimate execution duration
      - task: "Add mandatory/optional flags"
        effort: "1 hour"
        details: |
          Security assessments = mandatory
          Performance assessments = optional
          Critical failures block merge
    code_examples:
      - file: "src/agents/verification/assessment-engine.ts"
        lines: "200-415"
        pattern: |
          planAssessment(claim: Claim): AssessmentPlan {
            const assessments = [];
            if (this.matchesSecurityPattern(claim)) {
              assessments.push({
                type: 'Security',
                tool: 'semgrep',
                threshold: 0,
                mandatory: true
              });
              assessments.push({
                type: 'TestCoverage',
                tool: 'jest',
                threshold: 90,
                mandatory: true
              });
            }
            return { claim, assessments, priority: 'critical' };
          }
    lessons:
      - "✅ Security code requires 90%+ coverage (not 80%)"
      - "✅ Mandatory vs optional critical for blocking"
      - "✅ Tool-agnostic design enables easy swapping"

  phase3_logging:
    effort: "2 hours"
    description: "Log assessment plans to JSONL"
    tasks:
      - task: "Implement JSONL logging"
        effort: "1 hour"
        details: |
          Log to .versatil/verification/assessment-plans.jsonl
          Include claim, assessments, priority, reason
          Append-only format
      - task: "Add session tracking"
        effort: "1 hour"
        details: |
          Include session ID
          Timestamp each plan
          Enable trend analysis
    code_examples:
      - file: ".versatil/verification/assessment-plans.jsonl"
        pattern: |
          {"sessionId":"abc123","timestamp":"2025-10-22T17:30:00Z","claim":"Created src/api/auth/login.ts","priority":"critical","reason":"Security-sensitive code detected","assessments":[{"type":"Security","tool":"semgrep","mandatory":true,"threshold":0},{"type":"TestCoverage","tool":"jest","mandatory":true,"threshold":90}]}
    lessons:
      - "✅ Log BEFORE execution (audit trail)"
      - "✅ JSONL enables easy trend analysis"

  phase4_integration:
    effort: "1 hour"
    description: "Integrate with Victor-Verifier"
    tasks:
      - task: "Add to post-agent-response hook"
        effort: "1 hour"
        details: |
          After verification, check if assessment needed
          Generate assessment plan if needed
          Log plan to JSONL
    code_examples:
      - file: ".claude/hooks/post-agent-response.ts"
        lines: "350-416"
        pattern: |
          const verificationResult = await coveEngine.verify(claim);
          if (assessmentEngine.needsAssessment(claim)) {
            const plan = assessmentEngine.planAssessment(claim);
            await logAssessmentPlan(plan);
          }
    lessons:
      - "✅ Assessment detection after verification"
      - "✅ Phase 1 = planning only (no execution yet)"

success_metrics:
  - "✅ 71 keywords across 5 categories"
  - "✅ 8 assessment tools integrated"
  - "✅ Mandatory vs optional flags"
  - "✅ Assessment plans logged to JSONL"
  - "✅ Priority assignment (critical, high, medium, low)"
  - "⏳ Phase 2: Auto-execution (upcoming)"

learnings_applied_from:
  - pattern: "assessment-engine-v6.6.0"
    applied: "Pattern detection and assessment planning"
  - pattern: "victor-verifier-anti-hallucination"
    applied: "JSONL logging for audit trail"

common_pitfalls:
  - pitfall: "Semantic analysis too complex for Phase 1"
    solution: "Start with keyword matching, add semantics in Phase 2"
  - pitfall: "Hardcoded tool paths"
    solution: "Use npm scripts and configurable commands"
  - pitfall: "Synchronous blocking execution"
    solution: "Phase 1 = logging only, Phase 2 = async execution"
  - pitfall: "All-or-nothing assessments"
    solution: "Use mandatory vs optional flags"

estimated_vs_actual:
  estimated: "12 hours"
  actual: "14 hours"
  accuracy: "86%"
  variance_reason: "Pattern detection had more false positives than expected"

next_similar_feature:
  estimated_effort: "10 hours"  # 29% faster with this template
  confidence: "92%"
  code_reuse: "60%"  # Pattern detection logic reusable
  known_pitfalls: "5 avoided"

roadmap:
  phase2_execution:
    effort: "20 hours"
    description: "Auto-execute assessment tools and parse results"
    tasks:
      - "Tool execution engine"
      - "Output parsing (semgrep, jest, lighthouse, axe-core)"
      - "Result formatting"
      - "Integration with Maria-QA/Marcus-Backend/James-Frontend"

  phase3_quality_gates:
    effort: "15 hours"
    description: "Merge blocking for failed mandatory assessments"
    tasks:
      - "GitHub status checks integration"
      - "Pull request comments with results"
      - "Dashboard for assessment trends"

---

# Usage Notes

When implementing an assessment engine:

1. **Start with pattern detection**
   - Keyword matching for Phase 1
   - Semantic analysis for Phase 2
   - 5 categories: security, api, ui, test, database

2. **Security patterns are critical priority**
   - 0 vulnerabilities (mandatory)
   - 90%+ test coverage (mandatory)
   - No exceptions

3. **Log plans before execution**
   - Audit trail
   - Enables debugging
   - Supports trend analysis

4. **Tool-agnostic design**
   - Easy to swap tools (semgrep → other SAST)
   - Configuration-driven
   - npm scripts for execution

5. **Mandatory vs optional**
   - Security/critical = mandatory
   - Performance/nice-to-have = optional
   - Blocks merge on mandatory failures

**Phase**: Phase 1 Complete (Planning), Phase 2 Next (Execution)
**Template last updated**: 2025-10-22 (v6.6.0)
**Success rate**: 90% (1/1 implementations)
**Recommended for**: Quality automation, security auditing, assessment systems
