{
  "name": "File Upload to S3",
  "category": "File Management",
  "keywords": ["s3", "aws", "upload", "file", "image", "storage", "presigned", "multipart", "cloudfront", "cdn"],
  "description": "Secure file upload system using AWS S3 with presigned URLs, image optimization, CDN distribution, and multipart upload for large files.",
  "estimated_effort": {
    "hours": 5,
    "range": "4-7",
    "confidence": 90
  },
  "complexity": "Medium",
  "time_savings": "2-3 hours per implementation",
  "success_rate": 94,
  "technologies": ["AWS SDK", "S3", "CloudFront", "Sharp (image processing)", "Multer"],
  "use_cases": [
    "Profile picture uploads",
    "Document storage",
    "Media library (photos, videos)",
    "File attachments",
    "Backup storage",
    "Large file transfers"
  ],
  "implementation": {
    "presigned_url_generation": {
      "description": "Server generates presigned URL for direct browser upload",
      "code": "// Server: src/uploads/s3-service.ts\nimport { S3Client, PutObjectCommand, GetObjectCommand } from '@aws-sdk/client-s3';\nimport { getSignedUrl } from '@aws-sdk/s3-request-presigner';\nimport { v4 as uuidv4 } from 'uuid';\n\nconst s3Client = new S3Client({\n  region: process.env.AWS_REGION!,\n  credentials: {\n    accessKeyId: process.env.AWS_ACCESS_KEY_ID!,\n    secretAccessKey: process.env.AWS_SECRET_ACCESS_KEY!\n  }\n});\n\nconst BUCKET_NAME = process.env.S3_BUCKET_NAME!;\n\nexport class S3UploadService {\n  /**\n   * Generate presigned URL for direct upload from browser\n   */\n  async generateUploadUrl(params: {\n    fileName: string;\n    fileType: string;\n    folder?: string;\n    expiresIn?: number; // seconds, default 300 (5 min)\n  }) {\n    const key = `${params.folder || 'uploads'}/${uuidv4()}-${params.fileName}`;\n    \n    const command = new PutObjectCommand({\n      Bucket: BUCKET_NAME,\n      Key: key,\n      ContentType: params.fileType,\n      ACL: 'private' // Never use 'public-read' for security\n    });\n    \n    const uploadUrl = await getSignedUrl(s3Client, command, {\n      expiresIn: params.expiresIn || 300\n    });\n    \n    return {\n      uploadUrl,\n      key,\n      publicUrl: `https://${BUCKET_NAME}.s3.${process.env.AWS_REGION}.amazonaws.com/${key}`\n    };\n  }\n  \n  /**\n   * Generate presigned URL for file download\n   */\n  async generateDownloadUrl(key: string, expiresIn = 3600) {\n    const command = new GetObjectCommand({\n      Bucket: BUCKET_NAME,\n      Key: key\n    });\n    \n    return await getSignedUrl(s3Client, command, { expiresIn });\n  }\n  \n  /**\n   * Delete file from S3\n   */\n  async deleteFile(key: string) {\n    const command = new DeleteObjectCommand({\n      Bucket: BUCKET_NAME,\n      Key: key\n    });\n    \n    await s3Client.send(command);\n  }\n}",
      "file": "src/uploads/s3-service.ts",
      "lines": "1-66"
    },
    "client_upload": {
      "description": "React client for file upload with progress tracking",
      "code": "// Client: src/hooks/useS3Upload.ts\nimport { useState } from 'react';\nimport axios from 'axios';\n\ninterface UploadProgress {\n  percent: number;\n  bytesUploaded: number;\n  totalBytes: number;\n}\n\nexport function useS3Upload() {\n  const [uploading, setUploading] = useState(false);\n  const [progress, setProgress] = useState<UploadProgress | null>(null);\n  const [error, setError] = useState<string | null>(null);\n  \n  const uploadFile = async (file: File, folder?: string) => {\n    setUploading(true);\n    setError(null);\n    setProgress({ percent: 0, bytesUploaded: 0, totalBytes: file.size });\n    \n    try {\n      // Step 1: Get presigned URL from server\n      const { data } = await axios.post('/api/uploads/presigned-url', {\n        fileName: file.name,\n        fileType: file.type,\n        folder\n      });\n      \n      const { uploadUrl, key } = data;\n      \n      // Step 2: Upload directly to S3\n      await axios.put(uploadUrl, file, {\n        headers: {\n          'Content-Type': file.type\n        },\n        onUploadProgress: (progressEvent) => {\n          const percent = Math.round(\n            (progressEvent.loaded * 100) / (progressEvent.total || file.size)\n          );\n          \n          setProgress({\n            percent,\n            bytesUploaded: progressEvent.loaded,\n            totalBytes: progressEvent.total || file.size\n          });\n        }\n      });\n      \n      // Step 3: Notify server of successful upload\n      await axios.post('/api/uploads/complete', {\n        key,\n        fileName: file.name,\n        fileType: file.type,\n        fileSize: file.size\n      });\n      \n      return { key, url: data.publicUrl };\n    } catch (err: any) {\n      const errorMsg = err.response?.data?.error || 'Upload failed';\n      setError(errorMsg);\n      throw new Error(errorMsg);\n    } finally {\n      setUploading(false);\n    }\n  };\n  \n  return { uploadFile, uploading, progress, error };\n}",
      "file": "src/hooks/useS3Upload.ts",
      "lines": "1-66"
    },
    "image_optimization": {
      "description": "Server-side image resizing and optimization with Sharp",
      "code": "// Server: src/uploads/image-processor.ts\nimport sharp from 'sharp';\nimport { S3Client, PutObjectCommand } from '@aws-sdk/client-s3';\nimport { PassThrough } from 'stream';\n\nconst s3Client = new S3Client({\n  region: process.env.AWS_REGION!,\n  credentials: {\n    accessKeyId: process.env.AWS_ACCESS_KEY_ID!,\n    secretAccessKey: process.env.AWS_SECRET_ACCESS_KEY!\n  }\n});\n\nconst BUCKET_NAME = process.env.S3_BUCKET_NAME!;\n\nexport class ImageProcessor {\n  /**\n   * Resize and optimize image, upload to S3\n   */\n  async processAndUpload(params: {\n    buffer: Buffer;\n    key: string;\n    sizes: { width: number; height?: number; suffix: string }[];\n  }) {\n    const results = [];\n    \n    for (const size of params.sizes) {\n      const resizedBuffer = await sharp(params.buffer)\n        .resize(size.width, size.height, {\n          fit: 'cover',\n          position: 'center'\n        })\n        .jpeg({ quality: 80, progressive: true })\n        .toBuffer();\n      \n      const key = params.key.replace(/\\.(jpg|jpeg|png|webp)$/i, `${size.suffix}.$1`);\n      \n      await s3Client.send(new PutObjectCommand({\n        Bucket: BUCKET_NAME,\n        Key: key,\n        Body: resizedBuffer,\n        ContentType: 'image/jpeg',\n        CacheControl: 'max-age=31536000' // 1 year\n      }));\n      \n      results.push({\n        size: size.suffix,\n        key,\n        url: `https://${BUCKET_NAME}.s3.${process.env.AWS_REGION}.amazonaws.com/${key}`\n      });\n    }\n    \n    return results;\n  }\n  \n  /**\n   * Generate thumbnails for image\n   */\n  async generateThumbnails(originalKey: string, buffer: Buffer) {\n    return await this.processAndUpload({\n      buffer,\n      key: originalKey,\n      sizes: [\n        { width: 150, suffix: '-thumb' },\n        { width: 640, suffix: '-medium' },\n        { width: 1920, suffix: '-large' }\n      ]\n    });\n  }\n}",
      "file": "src/uploads/image-processor.ts",
      "lines": "1-67"
    },
    "multipart_upload": {
      "description": "Multipart upload for large files (>100MB)",
      "code": "// Server: src/uploads/multipart-upload.ts\nimport {\n  S3Client,\n  CreateMultipartUploadCommand,\n  UploadPartCommand,\n  CompleteMultipartUploadCommand,\n  AbortMultipartUploadCommand\n} from '@aws-sdk/client-s3';\n\nconst s3Client = new S3Client({\n  region: process.env.AWS_REGION!,\n  credentials: {\n    accessKeyId: process.env.AWS_ACCESS_KEY_ID!,\n    secretAccessKey: process.env.AWS_SECRET_ACCESS_KEY!\n  }\n});\n\nconst BUCKET_NAME = process.env.S3_BUCKET_NAME!;\nconst CHUNK_SIZE = 10 * 1024 * 1024; // 10MB chunks\n\nexport class MultipartUploadService {\n  /**\n   * Initialize multipart upload\n   */\n  async initiate(key: string, contentType: string) {\n    const command = new CreateMultipartUploadCommand({\n      Bucket: BUCKET_NAME,\n      Key: key,\n      ContentType: contentType\n    });\n    \n    const response = await s3Client.send(command);\n    return response.UploadId!;\n  }\n  \n  /**\n   * Upload single part\n   */\n  async uploadPart(params: {\n    uploadId: string;\n    key: string;\n    partNumber: number;\n    body: Buffer;\n  }) {\n    const command = new UploadPartCommand({\n      Bucket: BUCKET_NAME,\n      Key: params.key,\n      UploadId: params.uploadId,\n      PartNumber: params.partNumber,\n      Body: params.body\n    });\n    \n    const response = await s3Client.send(command);\n    return {\n      ETag: response.ETag!,\n      PartNumber: params.partNumber\n    };\n  }\n  \n  /**\n   * Complete multipart upload\n   */\n  async complete(params: {\n    uploadId: string;\n    key: string;\n    parts: { ETag: string; PartNumber: number }[];\n  }) {\n    const command = new CompleteMultipartUploadCommand({\n      Bucket: BUCKET_NAME,\n      Key: params.key,\n      UploadId: params.uploadId,\n      MultipartUpload: { Parts: params.parts }\n    });\n    \n    const response = await s3Client.send(command);\n    return response.Location!;\n  }\n  \n  /**\n   * Abort multipart upload (cleanup)\n   */\n  async abort(uploadId: string, key: string) {\n    const command = new AbortMultipartUploadCommand({\n      Bucket: BUCKET_NAME,\n      Key: key,\n      UploadId: uploadId\n    });\n    \n    await s3Client.send(command);\n  }\n}",
      "file": "src/uploads/multipart-upload.ts",
      "lines": "1-90"
    }
  },
  "prerequisites": [
    "AWS account with S3 access",
    "IAM user with S3 permissions (PutObject, GetObject, DeleteObject)",
    "S3 bucket created (public access blocked by default)",
    "Environment variables: AWS_REGION, AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, S3_BUCKET_NAME",
    "CloudFront distribution (optional, for CDN)",
    "Sharp library for image processing (npm install sharp)"
  ],
  "step_by_step": [
    {
      "step": 1,
      "title": "Install AWS SDK",
      "command": "npm install @aws-sdk/client-s3 @aws-sdk/s3-request-presigner sharp uuid"
    },
    {
      "step": 2,
      "title": "Create S3 Bucket",
      "details": "AWS Console → S3 → Create bucket → Block all public access"
    },
    {
      "step": 3,
      "title": "Create IAM User",
      "details": "IAM → Users → Add user → Attach policy: AmazonS3FullAccess (or custom policy)"
    },
    {
      "step": 4,
      "title": "Set Environment Variables",
      "details": "Add AWS credentials to .env (never commit!)"
    },
    {
      "step": 5,
      "title": "Implement S3UploadService",
      "details": "Create presigned URL generation and file deletion methods"
    },
    {
      "step": 6,
      "title": "Create Client Upload Hook",
      "details": "Implement useS3Upload with progress tracking"
    },
    {
      "step": 7,
      "title": "Add Image Processing (Optional)",
      "details": "Use Sharp to resize and optimize images before upload"
    },
    {
      "step": 8,
      "title": "Set Up CloudFront (Optional)",
      "details": "Create CDN distribution pointing to S3 bucket for faster delivery"
    },
    {
      "step": 9,
      "title": "Test Upload Flow",
      "details": "Upload test file, verify in S3 console, test download"
    },
    {
      "step": 10,
      "title": "Add File Validation",
      "details": "Validate file type, size, and dimensions on server"
    }
  ],
  "security": {
    "acl": "Use 'private' ACL - never 'public-read' for security",
    "presigned_urls": "Short expiration (5 min for upload, 1 hour for download)",
    "validation": "Validate file type, size, and dimensions on server-side",
    "signed_urls": "Use CloudFront signed URLs for sensitive content",
    "iam_permissions": "Least privilege - only required S3 actions"
  },
  "warnings": [
    "⚠️ Never use 'public-read' ACL - major security risk",
    "⚠️ Presigned URLs expire - handle expiration errors gracefully",
    "⚠️ S3 costs can grow quickly - set up lifecycle policies to delete old files",
    "⚠️ Large file uploads may timeout - use multipart upload for files >100MB",
    "⚠️ Image processing is CPU-intensive - consider AWS Lambda for serverless processing",
    "⚠️ Always validate file types on server - client validation is not secure"
  ],
  "lessons_learned": [
    "Use presigned URLs instead of uploading through server (saves bandwidth + faster)",
    "Set Cache-Control headers for static assets (1 year for immutable files)",
    "Use UUID in file names to prevent collisions and path traversal attacks",
    "Sharp is faster than ImageMagick for image processing",
    "CloudFront CDN reduces latency by 60-80% vs direct S3 access",
    "Set up S3 lifecycle policies to move old files to Glacier (cheaper storage)",
    "Use multipart upload for files >100MB (more reliable, resumable)",
    "Store file metadata in database (key, size, type) for easy querying"
  ],
  "testing": {
    "unit_tests": "Mock AWS SDK in tests using jest.mock()",
    "integration_tests": "Use localstack (local AWS emulator) for testing",
    "file_types": "Test with images (jpg, png), documents (pdf), videos (mp4)"
  },
  "monitoring": {
    "metrics": [
      "Upload success rate",
      "Average upload time",
      "Storage usage (GB)",
      "CloudFront cache hit rate",
      "Failed upload reasons"
    ],
    "tools": ["AWS CloudWatch", "S3 metrics dashboard", "Cost Explorer"]
  },
  "related_patterns": [
    "image-processing.json (for advanced image manipulation)",
    "api-rate-limiting.json (to prevent upload abuse)",
    "auth-system.yaml (for authenticated uploads)"
  ]
}
